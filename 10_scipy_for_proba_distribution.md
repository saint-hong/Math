# 사이파이로 공부하는 확률분포

### 확률분포
- 이산확류분포 : 범줏값 출력
   - 베르누이분포
   - 이항분포
   - 카테고리분포
   - 다항분포
- 연속확률분포 : 연속적인 값 출력
   - 정규분포
   - 정규분포와 중심극한정리
   - 정규분포와 통계량
   - 데이터간의 상관관계 파악 : 스튜던트t분포, 카이제곱분포, F분포
   - 베이즈 추정 : 베타분포, 감마분포, 디리클레분포

## 확률분포 분석
- scipy 패키지의 stats 서브패키지로 구현
- scipy.stats.클래스이름(모수, 모수, ...)
- 확률분포 객체 생성 : 분포 선택 
    - rv = scipy.stats.norm()
- 모수 지정 : 분포의 구체적인 모양 선택
    - loc : 기댓값, scale : 표준편차
- 객체의 매서드 호출 : 분포에서 어떤 값이 자주 나오는지 알려주는 확률분포함수 선택
    - rv.pdf(), rv.cdf(), rv.rvs(size, random_state), rv.pmf(), ...
    
### 확률분포 클래스
- 사이파이의 stats 서브패키지에서 다양한 확률분포 클래스를 사용할 수 있다.
- 이산확률분포 : 분류문제 classification
    - bernoulli : 베르누이 분포
    - binom : 이항분포
    - multinomial : 다항분포
- 연속속확률분포 : 회귀분석 regression analysis
    - uniform : 균일분포
    - norm : 정규분포
    - beta : 베타분포
    - gamma : 감마분포
    - t : 스튜던트 t분포
    - chi2 : 카이제곱분포
    - f : F 분포
    - dirichlet : 디리클리분포
    - multivariate_normal : 다변수 정규분포

### 모수지정
- 확률분포 객체의 생성시 분포의 형상을 구체적으로 지정하는 모수를 입력해야한다.
- 각 확률분포마다 모수가 다르다.
- 일반적으로 비슷한 것 : loc : 기댓값, scale : 표본표준편차
    - sp.stats.norm(loc=1, scale=2)
    
### 확률분포 매서드
- 확률분포 객체가 가지는 매서드
    - pmf : 확률질량함수 probability mass function
    - pdf : 확률밀도함수 probability density function
    - cdf : 누적분포함수 cumulative distribution function
    - ppf : 누적분포함수의 역함수 inverse cumulative distribution function
    - sf : 생존함수 survival function = 1 - cdf
    - isf : 생존함수의 역함수 inverse survival function
    - rvs : 랜덤 표본 생성 random value sampling
    - logpmf : 로그 확률질량함수
    - logcdf : 로그 누적분포함수
    - logsf : 로그 생존함수
    - stats : mean, variance, skew, kurtois 값 반환
    - median : 분포의 표본중앙값
    - mean : 분포의 표본평균
    - var : 분포의 표본분산 
    - std : 분포의 표본표준편차
    - interval : 신뢰구간 
- 사용법 : 확률분포 객체뒤에 매서드를 붙이고 x값을 넣는다.
    - rv.pdf(x), rv.pmf(x), rv.cdf(x), rv.stats(), rv.mean(), rv.median(), rv.var(), ... 
    
### 누적분포함수 
- 표본값 x에 대한 누적확률 출력

### 무작위 표본 생성
- 무작위로 표본을 만들어 주는 매서드
    - rvs(size=1000, random_state=0)
- 인수
    - size=(2, 3) : 2X3 행렬
    - size=1000 : 1X1000 행렬 (열벡터)
    
### 변환 확률변수의 시뮬레이션
- ```변환 transform``` : 기존의 확률변수를 사용하여 새로운 확률변수를 만드는 것
    - Y = f(X), {x_1, x_2, x_N} -> {f(x_1), f(x_2), f(x_N)}, {x_i} -> {y_i}
    - Y = X1 + X2 + X3 + ... (X1, X2, X3 ... 는 모두 X의 복사본과 같다)
    - 여러 확률변수가 있을 때도 성립 : Z = X + Y
- 시뮬레이션 기능을 사용하여 변환 확률변수의 확률분포를 알 수 있다. (시뮬레이션이 특별한 기능이라기 보다 확률변수의 변환과정을 의미하는 것 같다.)

## 베르누이분포와 이항분포
- 베르누이분포와 이항분포는 확률분포 중 가장 단순한 분포이지만 분류문제 classification 에서 널리 사용된다.

### 베르누이 시행
- ```베르누이 시행 bernoulli trial``` : 결과가 두 가지 중 하나만 나오는 실험, 시행
    - 동전을 한 번 던져 결과가 앞면 또는 뒷면이 나오는 실험
    - 바둑알을 하나 선택할 때 결과가 흑돌 또는 백돌이 나오는 실험

### 베르누이 확률변수
- ```베르누이 확률변수 bernoulli random variable``` : 베르누이 실험 결과를 0 또는 1로 바꾼 것.
    - 정수 0, 1 둘 중에 하나의 값만 가질 수 있으므로 이산확률변수이다.
    - 정수 -1, 1로 나타내기도 한다. 
    
### 베르누이 확률분포
- ```베르누이 확률분포``` : 베르누이 확률변수의 분포
    - $X \sim Bern(x;\mu)$ : 확률변수 X가 베르누이 분포를 따른다. 
    - \mu : 1이 나올 확률
    - $Bern(x;\mu) = \left\{\begin{matrix}
\mu \;\;\;\;\;\;\;\;\; if\;\;x=1 \\ 
1-\mu \;\;\; if\;\;x=0 \\
\end{matrix}\right.$
- 한줄의 수식으로 바꾸면
    - $Bern(x;\mu) = \mu^{x}(1-\mu)^{(1-x)}$
- 베르누이 확률변수의 표본값이 -1, 1 일때 수식
    - $Bern(x;\mu) = \mu^{(1+x)/2}(1-\mu)^{(1-x)/2}$

### 사이파이를 사용한 베르누이 확률변수의 시뮬레이션
- 사이파이의 stats 패키지의 bernoulli 클래스를 사용하여 베르누이 확률변수 객체를 만든다.
    - sp.stats.bernoulli(p=mu) : 모수 mu의 값을 설정 : mu는 1이 나올 확률
    - 앞면이 나올 확률이 0.6, 뒷면이 나올 확률이 0.4인 동전과 같다.
- ```시뮬레이션 simulation``` : 확률변수의 표본을 생성하는 작업
    - 베르누이 확률분포 객체를 사용하여 표본 100개를 만드는 것은 동전을 100번 던져서 나온 결과와 같은 의미이다.
    - rv.rvs(100, random_state=0)
    
### 베르누이분포의 모멘트
- **베르누이분포의 기댓값**
    - $\text{E}[X] = \mu$
    - 베르누이분포의 기댓값은 mu이다. (mu=1이 나올 확률, 모수값)
- 증명
    - 이산확률변수 X의 기댓값 공식 사용
    - 1과 0을 대입하여 계산
    - $\begin{aligned}
\text{E}[X] 
&= \sum_{x_i \in \Omega} x_i p(x_i) \\
&= 1 \cdot \mu + 0 \cdot (1-\mu) \\
&= \mu
\end{aligned}$

- **베르누이분포의 분산**
    - $\text{Var}[X] = \mu \cdot (1-\mu)$
- 증명
    - 이산확률변수 X의 분산 공식 사용
    - 0과 1을 대입하여 계산
    - $\begin{aligned}
    \text{Var}[X] = 
    &= \sum_{x_i \in \Omega} (x_i - \bar{x})^2 p(xi) \\
    &= \sum_{x_i \in \Omega} (x_i - \mu)^2 p(xi) \\
    &= (0 - \mu)^2 p(0) + (1 - \mu)^2 p(1) \\
    &= (-\mu)^2 \cdot (1-\mu) + (1-\mu)^2 \cdot \mu \\
    &= \mu(1-\mu)(\mu + (1-\mu)) \\
    &= \mu(1-\mu) \\
    \end{aligned}$
    
### 이항분포
- ```이항분포 binomial distribution``` : N번 시행해서 X번 성공하여 성공할 확률이 \mu인 확률변수의 확률분포
    - $X \sim Bin(x;N,\mu)$
    - 확률변수 X는 성공한 횟수이므로 0~N 까지의 정수 중 하나이다.
- 베르누이분포와 이항분포는 모두 베르누이 확률변수에서 나온 분포이다.
    - 표본데이터가 1개이면 베르누이분포, 표본데이터가 여러개이면 이항분포이다.
- ```이항분포 확률변수 X의 확률질량함수 pmf```
    - 베르누이 확률분포를 따르는 확률변수 Y : $Y \sim Bern(x;\mu)$
    - 확률변수 Y의 N개의 표본 : y_1, y_2, ..., y_N (표본 각각 성공 아니면 실패 값을 갖는다.)
    - 성공한 횟수 x : $x = \sum_{i=1}^{N} y_i$ (Y의 표본값의 합)
    - 베르누이 확률분포를 따르는 확률변수 Y의 확률질량함수를 대입하여 정리
    - $Bin(x;N,\mu) = \binom{N}{x} \mu^{x} (1-\mu)^{N-x}$
        - $\binom{N}{x}=\dfrac{N!}{x!(N-x)!}$ 는 N개 중에서 순서 상관 없이 x개를 선택할 경우의 수 : **조합 combination**
        - $N! = N \cdot (N-1) \cdot (N-1) \cdots 2 \cdot 1$ : **팩토리얼 factorial**

### 이항분포의 모멘트
- **이항분포의 기댓값** 
    - $\text{E}[X] = N\mu$
    - 이항분포의 기댓값은 전체시행횟수 * 모수값과 같다. (mu : 1이 나올 확률)
- 증명
    - $\begin{aligned}
    \text{E}[X]
    &= \text{E}[\sum_{i=1}^{N} Y_i] \\
    &= \sum_{i=1}^{N} \text{E}[Y_i] \;\; ((sol:E[Y_i]=E[Y], copy))\\
    &= N\mu \;\; ((sol:E[bern]=\mu))
    \end{aligned}$
    - Y_i는 서로 독립인 베르누이분포이다. 베르누이분포의 기댓값은 mu이다.
- **이항분포의 분산**
    - $\text{Var}[X] = N\mu(1-\mu)$
- 증명
    - $\begin{aligned}
    \text{Var}[X]
    &= \text{Var}[\sum_{i=1}^{N} Y_i] \\
    &= \sum_{i=1}^{N} \text{Var}[Y_i] \;\; ((sol:Var[Y_i]=Var[Y], copy)) \\
    &= N\mu(1-\mu) \;\; ((sol:Var[bern]=\mu(1-\mu)))
    \end{aligned}$
    - Y_i는 서로 독립인 베르누이분포이다. 베르누이분포의 분산값은 mu(1-mu)이다.
    
### 베르누이분포와 이항분포의 모수추정
- ```모수추정 parameter estimation``` : 데이터에서 모수값을 찾아내는 과정
    - $\hat{\mu} = \dfrac{\sum_{i=1}^{N}x_i}{N} = \dfrac{N_1}{N}$
    - N : 전체 데이터의 수, N_1 : 1이 나온 횟수
- 베르누이분포와 이항분포는 공통된 모수 mu를 갖는다.
- 일반적으로 데이터의 표본이 1개 보다 많기 때문에 이항분포가 된다. 

### 베르누이분포의 활용
- 베르누이분포는 2가지 측면에서 활용할 수 있다. 
- **베이지안 관점** : 분류예측 문제에서 출력데이터가 두 값으로 구분되는 카테고리값인 경우, 두 값중 어느 것이 **나올 가능성**이 높은지를 표현하는 데 사용된다.
- **빈도주의 관점** : 입력데이터가 0 또는 1 혹은 참 또는 거짓 등 두 개의 값으로 구분되는 카테고리값인 경우, 두 종류의 값이 **나타나는 비율**을 표현하는 데 사용된다.

### 스팸메일 필터링에서 베르누이분포의 활용
- 스팸메일과 정상메일을 구분하는 필터 : 전체 메일 10개 중 스팸메일이 6개라면 이 메일 계정으로 들어오는 메일이 스팸메일일 가능성은 60%라고 볼 수 있다.
    - 모수가 0.6인 베르누이분포와 같다. $\mu = 0.6$
    - $p(y) = \text{Bern}(y;\mu=0.6)$
- 스팸메일 필터에서 베르누이분포를 활용하는 방법, 스팸메일은 특정한 단어를 가지고 있을 확률이 높다. 따라서 어떤 키워드가 있는지 없는지를 BOW Bag of Words 방식으로 인코딩된 벡터로 나타낼 수 있다. 
    - $x = \left[\begin{matrix}
1 \\
0 \\
1 \\
0 
\end{matrix}\right]$
    - 키워드가 있으면 1, 없으면 0으로 표시된다. 1번과 3번 키워드는 있고, 2번과 4번 키워드는 들어있지 않다.
- 여러개의 메일을 키워드가 있는지 없는지 조사하여 BOW 벡터로 나타내면,
    - $X_{spam} = \left[\begin{matrix}
    1 & 0 & 1 & 0 \\
    1 & 1 & 1 & 0 \\
    1 & 1 & 0 & 1 \\
    0 & 0 & 1 & 1 \\
    1 & 1 & 0 & 0 \\
    1 & 1 & 1 & 1 \\
    \end{matrix}\right]$
    - 열은 키워드의 종류, 행은 메일을 의미한다. 2번째 행은 2번째 메일에 어느 키워드가 있는지 없는지를 나타낸다.
- 스팸메일의 특성을 베르누이 확률변수의 튜플로 나타낼 수 있다. 
    - X1 : 스팸 메일(Y=1)이 첫번쨰 키워드를 가지고 있으면 1, 없으면 0이 되는 확률변수
        - $p(X_1 = 1 | Y = 1) = Bern(x_1 ; \mu_{spam, 1})$
    - X2 : 스팸 메일(Y=1)이 첫번쨰 키워드를 가지고 있으면 1, 없으면 0이 되는 확률변수
        - $p(X_2 = 1 | Y = 1) = Bern(x_2 ; \mu_{spam, 2})$
    - X1 : 스팸 메일(Y=1)이 첫번쨰 키워드를 가지고 있으면 1, 없으면 0이 되는 확률변수
        - $p(X_3 = 1 | Y = 1) = Bern(x_3 ; \mu_{spam, 3})$
    - X1 : 스팸 메일(Y=1)이 첫번쨰 키워드를 가지고 있으면 1, 없으면 0이 되는 확률변수
        - $p(X_4 = 1 | Y = 1) = Bern(x_4 ; \mu_{spam, 4})$
- 모수추정을 사용하여 각 베르누이 확률분포의 모수값을 추정할 수 있다.
    - $\mu_{spam, 1} = \dfrac{5}{6},\;\;\mu_{spam, 2} = \dfrac{4}{6},\;\;\mu_{spam, 3} = \dfrac{3}{6},\;\;\mu_{spam, 4} = \dfrac{3}{6}$
    - 1이 나오는 경우 / 전체 데이터수
    - 키워드 1이 나오는 경우 / 전체 메일 수

## 카테고리분포와 다항분포
- 베르누이분포의 확장판 : 카테고리분포
- 이항분포의 확장판 : 다항분포
- 베르누이분포 : **이진분류 문제 binary classification** 에 사용
- 카테고리분포 : **다중분류 multi classification** 에 사용

### 카테고리 확률변수
- 베르누이 확률변수는 0 또는 1, 1 또는 -1 이 나오는 확률변수이다.
- `카테고리 확률변수 categorical random variable` : 1 부터 K까지 K개 정숫값 중 하나가 나오는 확률변수이다.
    - 이러한 정숫값을 범줏값, 카테고리, 클래스라고 한다. 
- 카테고리 확률변수의 정숫값은 스칼라값이지만 **원핫인코딩 one-hot-encoding**으로 나타낸 벡터가 된다.
    - x = 1 -> x = (1, 0, 0, 0, 0, 0)
    - x = 2 -> x = (0, 1, 0, 0, 0, 0)
    - x = 3 -> x = (0, 0, 1, 0, 0, 0)
    - x = 4 -> x = (0, 0, 0, 1, 0, 0)
    - x = 5 -> x = (0, 0, 0, 0, 1, 0)
    - x = 6 -> x = (0, 0, 0, 0, 0, 1)
    - K = 100 이면 100차원의 벡터로 표시된다. 
- 확률변수의 값 : x = (x1, x2, x3, x4, x5, x6), 이 벡터 x의 원소의 제한 조건 2가지
    - $x_i = \left\{\begin{matrix}
    0 \\
    1 \\
    \end{matrix}\right.$, x_k 값은 0 또는 1만 가능하다는 의미이다.
    - $\sum_{k=1}^{K} x_k = 1$, 모든 x_k는 0, 1 둘중 하나만 가능하고 총합이 1이므로 xk 중 하나만 1이라는 의미이다.
- 확률변수의 값 x1, x2, ...는 모두 0, 1의 값을 갖는 베르누이 확률이므로 모수를 갖는다. 
    - $\mu = (\mu_1, \mu_2, \cdots, \mu_k)$
    - 모수 벡터의 원소들도 제한 조건 2가지를 갖는다.
    - $\mu = \left\{\begin{matrix}
    0 \leq \mu_k \leq 1 \\
    \end{matrix}\right.$, 모수는 0과 1사이의 실수값을 갖는다.
    - $\sum_{k=1}^{K} \mu_{k} = 1$, 모든 모수가 0과 1사이의 실수값이면서 총합이 1이므로 모든 경우의 확률이 1이라는 의미이다.

### 카테고리 확률분포
- `카테고리 확률분포 categorical probability distribution` : 카테고리 확률변수의 확률분포
    - $\text{Cat}(x_1, x_2, \cdots, x_k ; \mu_1, \mu_2, \cdots, \mu_k)$
    - x와, \mu의 벡터로 묶으면
    - $\text{Cat}(x;\mu)$, (x=(x1, x2, ..., xk), \mu=(mu1, mu2, ..., muk), 둘다 열벡터)
- 카테고리 확률변수의 확률질량함수 pmf
    - $\text{Cat}(x;\mu) = \left\{\begin{matrix}
    \mu_1\;\;\;\;if x=(1,0,0,\cdots,0) \\
    \mu_2\;\;\;\;if x=(0,1,0,\cdots,0) \\
    \mu_3\;\;\;\;if x=(0,0,1,\cdots,0) \\
    \vdots \\
    \mu_k\;\;\;\;if x=(0,0,0,\cdots,1) \\
    \end{matrix}\right.$ 
    > $\text{Cat}(x;\mu) = \mu_{1}^{x_1}\mu_{2}^{x_2}\cdots\mu_{K}^{x_K} = \prod_{k=1}^{K} \mu_{k}^{x_K}$
- 카테고리 확률변수의 확률질량함수는 베르누이 확률질량함수와 같다.
    - 카데고리 확률변수의 원소의 제한조건과 이 원소들의 모수의 제한조건을 사용하면 증명된다.
    - $\text{Cat}(x;\mu) = \mu^{x}(1-\mu)^{1-x}$
    
### 카테고리 확률분포의 모멘트
- 카테고리 확률분포의 기댓값
    - $\text{E}[x_k] = \mu_{k}$, (베르누이 분포의 기댓값과 같으나 k번째 모수의 값이다.)
- 카테고리 확률분포의 분산값
    - $\text{Var}[x_k] = \mu_{k} (1-\mu_{k})$, (베르누이 분포의 분산값과 같으나 k번째 모수의 값이다.)
- 표본값 x_k가 열벡터이므로 기댓값과 분산값도 열벡터이다.

### 다중 분류 문제
- `다중분류 문제` : 예측할 범주값이 두 가지 이상인 경우에 해당한다. 이러한 경우는 **카테고리 분포**를 사용하여 범주값 데이터 모형을 만들 수 있다.
    - `이진분류 문제` : 예측할 범주값이 두 가지인 경우에 해당한다. 이러한 경우는 **베르누이 분포**를 사용하여 범주값 데이터 모형을 만들 수 있다.
- 붓꽃 데이터에서 범주값에 해당하는 데이터는 품종이다. 붓꽃 데이터의 품종값은 K=3인 카테고리분포를 따른다. 

### 다항분포
- `다항분포 multinomial distribution` : 카테고리 확률변수의 데이터가 여러개 있을때 데이터의 합은 다항분포가 된다.
    - 즉 카테고리 변수에서 나온 표본이 여러개 있을 때, 1~6 눈금이 있는 주사위를 여러번 던졌을 때 다항분포를 따른다.
    - 베르누이 확률변수의 데이터가 여러개 있을때 데이터의 합은 이항분포가 된다. 
- 카테고리가 K개인 카테고리 확률변수의 표본 데이터를 N개 얻었을 때 각각의 카테고리 k가 각각 x_k번 나올 확률분포를 의미한다.
    - 표본값이 벡터 x = (x1, x2, ..., xk)가 되는 확률분포를 말한다.
    - x = (1, 2, 1, 2, 3, 1) 이면 1+2+1+2+3+1=10번 시행중에, k=1이 1번, k=2가 2번, k=3이 1번, k=4가 2번, k=5가 3번, k=6이 1번 나온 것과 같다. 
    - 다항분포의 표본값의 합은 N (시행, 실험 횟수) 이다.
- 다항분포의 확률질량함수 pmf
    - $\text{Mu}[x;N,\mu] = \binom{N}{x} \prod_{k=1}^{K} \mu_{k}^{x_k} = \binom{N}{x_1,\cdots,x_k} \prod_{k=1}^{K} \mu_{k}^{x_k}$
    - $\binom{N}{x_1,\cdots,x_k} = \dfrac{N!}{x_{1}! \cdots x_{k}!}$
    - **다항분포의 확률질량함수는 이항분포의 확률질량함수와 같다.** (표본의 합=N, 모수 \mu의 합=1 이라는 조건을 사용하여 정리)

## 정규분포와 중심극한정리

### 정규분포와 가우스 정규분포
- `정규분포 normal distribution`과 `가우스 정규분포 Gaussian normal distribution` : 자연현상의 숫자를 확률모형으로 구현할 때 사용하는 분포
- 정규분포의 확률밀도 함수
    - $\mathcal{N}(x;\mu, \sigma^2) = \dfrac{1}{\sqrt{2 \pi \sigma^2}} \exp \left(- \dfrac{(x-\mu)^2}{2 \sigma^2} \right)$
    - 평균(mu)과 분산(sigma^2) 두 모수만으로 정의된다.
- 분산의 역수
    - `정밀도 precision` : $\beta = \dfrac{1}{\sigma^2}$, (머신러닝의 예측모형의 성능 지표에서 나옴)
- `표준 정규분포 standard normal distribution` : 평균=0, 분산=1 인 정규분포
- 정규분포의 확률밀도함수의 성질
    - $x=\mu$ 에서 확률밀도가 가장 크다. (평균값의 위치에서 확률밀도값이 가장 크다)
    - $x=\infty, x=-\infty$ 로 다가갈 수록 확률밀도가 작아진다.
    - **확률밀도함수는 누적분포함수의 도함수이고, 확률밀도함수에서의 확률은 면적과 같다.**
- 정규분포 확률변수에서 데이터 집합을 생성하여 히스토그램을 보면 정규분포 형태가 아닌것 처럼 보인다. 그런데 정규분포를 따르는게 맞다.
    - 반대로 어떤 현실 데이터 집합이 정규분포가 아니라고 해도 확률변수는 정규분포 일 수 있다는 의미로 해석한다.

### 로그정규분포
- 나스닥 주가의 수익률 그래프가 정규분포라면, 주가 자체는 어떤 분포일까?
- `로그정규분포 log-normal distribution` : 데이터에 로그를 한 값 또는 변화율(수익률)이 정규분포가 되는 분포를 말한다.
    - $ln(X) \sim \mathcal{N}(x;\mu, \sigma^2)$
    - **로그정규분포를 띄는 분포는 항상 양수이다.** 그러므로 로그변환을 한 다음 사용하는 것이 일반적이다.
- 로그정규분포의 확률밀도함수
    - $f(x;\mu, \sigma^2) = \left\{\begin{matrix}
\dfrac{1}{x \sigma \sqrt{2\pi}} \exp \left(- \dfrac{(ln x - \mu)^2}{2\sigma^2}\right)\;\;\;(x \geq 0) \\ 
0 \;\;\;(x \leq 0) \\
\end{matrix}\right.$
- 위키피디아 참조

### Q-Q플롯
- `Q-Q 플롯 quantile-quantile` : 분석할 표본 데이터의 분포와 정규분포의 분포 형태를 비교하여 표본 데이터가 정규분포를 따르는지 검사해주는 시각적 도구이다.
    - 어떤 확률변수의 분포가 정규분포인지 아닌지 확인하는 것은 통계적 분석에서 중요하다.
- Q-Q 플롯은 동일 분위수에 해당하는 정상 분포의 값과 주어진 데이터값을 한 쌍으로 만들어 그린 **스캐터 플롯scatter plot**과 같다.
- Q-Q 플롯을 그리는 과정
    - 표본 데이터를 정렬한다.
    - 표본 데이터 하나가 **전체 데이터 중의 몇 % 정도에 해당하는지 위칫값**을 구한다.
        - 위치값은 순서통계량(order statistics) 값을 사용한다. 
    - **표본정규분포의 분위함수 quantile function** : 확률값에 대한 누적확률함수의 역함수값을 구한다. 각 표본데이터의 위치값이 정규분포의 누적확률함수 cdf값이 되는표준 정규분포의 표본값을 구하는 것과 같다. 
    - 정렬된 표본데이터와 그에 대응하는 분위수(theoretical quantiles)를 하나의 쌍으로 하는 점을 2차원 공간에 그린다.
    - 모든 표본에 대해서 이 작업을 하여 나타낸다.
    - 표본이 1%에 해당한다면 F^(-1)(0.01) = -2.326 이 값이 분위함수의 값이다.
- 정규분포를 따르는 데이터 표본을 Q-Q 플롯으로 그리면 직선의 형태가 된다.
- 정규분포를 따르지 않는 데이터 표본을 Q-Q 플롯으로 그리면 직선의 양 끝이 휘어진 형태가 된다.
- 사이파이 패키지의 stats 서브패키지에는 Q-Q플롯을 그려주는 probplot() 명령이 있다.

### 중심극한정리
- `중심극한정리 central limit theorem` : 여러 확률변수의 합이 정규분포와 비슷한 분포를 이루는 현상
    - central은 확률이론의 중심이라는 뜻으로, 중요하다는 의미이다.
    - 중심극한정리에 의해서 실세계에서 발생하는 많은 현상을 정규분포를 이용해서 모형화할 수 있다. 
- 기댓값이 \mu, 분산이 \sigma^2 인 확률변수 X1, X2,...,X_N이 서로 독립일 때, 각 확률변수에서 표본 데이터 x1, x2, ..., xn을 뽑으면 평균은 다음과 같다.
    - $\bar{x}_{n} = \dfrac{1}{N}(x_1, x_2, \cdots, x_N)$
    - 이러한 평균값도 예측할 수 없는 확률변수이다. $\bar{X}_N$
- `중심극한정리` : N개의 **임의의 분포**로부터 얻은 표본의 평균은 N이 증가할 수록 기댓값이 \mu, 분산이 \sigma^2 / N 인 **정규분포로 수렴**한다. 
    - $\bar{X}_{N} \overset{d}{\rightarrow} \mathcal{N}(x;\mu, \dfrac{\sigma^2}{N})$
    - d ->는 N이 커질수록 특정한 분포의 형태로 수렴한다는 의미
- 정규화 : N개의 **임의의 분포**로부터 얻은 표본의 평균을 **정규화**하면 N이 증가할 수록 기대값이 0, 분산이 1인 **표준정규분포로 수렴**한다.
    - $\dfrac{\bar{X}_{N} - \mu}{\dfrac{\sigma}{\sqrt{N}}} \overset{d}{\rightarrow} \mathcal{N}(x;0, 1)$

### 정규분포의 통계량 분포
- 중심극한정리와 다르게 임의의 분포가 N개가 아닌 N개의 정규분포로부터 얻은 표본 평균은 어떤 분포가 될까?
- N개의 정규분포로부터 얻은 표본의 합은 기대값이 N\mu, 분산이 N\sigma^2 인 정규분포이다.
    - $x_i \sim \mathcal{N}(x;\mu, \sigma^2) \rightarrow \sum_{i=1}^{N}x_i \sim \mathcal{N}(x;N\mu, N\sigma^2)$
- 정규화하면 표준정규분포가 된다.
    - $x_i \sim \mathcal{N}{x;\mu, \sigma^2} \rightarrow z=\dfrac{\bar{x}-\mu}{\dfrac{\sigma}{\sqrt{N}}} \sim \mathcal{N}(x;0,1)$
- `z 통계량` : 정규분포의 표본의 평균
    - 중심극한정리는 N이 증가할 수록 표준정규분포로 수렴한다. N이 무한대가 되기전에는 정확한 정규분포가 아니다.
    - **z통계량은 N에 상관없이 항상 정확하게 표준정규분포이다.**

### 선형회귀 모형과 정규분포
- 정규분포는 선형회귀 모형에서 **잡음 disturbance** 을 모형화는데 사용된다.
    - 잡음이 예측값에 영향을 줄 수 있기 때문에 잡음을 모형화하는 작업 필요
- 선형회귀모형 : 입력변수(데이터 표본)이 종속변수에 선형적으로 영향을 미치는 모형
    - $\hat{y} = w_1x_1 + \cdots + w_Nx_N \approx y$
- `잡음 disturbance` : 데이터 분석에 있어서 측정할 수 없는 값을 의미한다.
    - $y = w_1x_1 + \cdots + w_Nx_N + \epsilon $
    - $y = w_1x_1 + \cdots + w_Nx_N + w_{N+1}x_{N+1} + w_{N+2}x_{N+2} + \cdots +  $
    - 예측값과 실제값의 차이인 잔차 residual 과는 다른 개념이다.
    - 선형회귀 모형을 만들 때 영향력이 작거나 일일이 측정하기 어려운 값들을 하나로 합친 것을 의미한다.
    - N번째 이후의 입력데이터들은 사실상 영향력이 미미하다.
    - 집값 데이터의 여러가지 피쳐들 중 귀신이 나오는지, 경비실이 있는지, 벌레가 몇마리 나오는지 등은 측정하기 어렵다. 이러한 요소들을 하나로 합친것
- 중심극한 정리에 의해서 임의의 확률변수의 합은 정규분포와 비슷한 형태이다.
    - $\epsilon = w_1x_1 + w_2x_2 + \cdots$
    - $\epsilon \sim \mathcal{N}(0, \sigma^2)$ : 기대값이 0인 정규분포라고 가정할 수 있다.    

# 스튜던트t분포, 카이제곱분포, F분포
- 연속확률분포
- 정규분포에서 파생된 분포들
    - 정규분포에서 생성된 데이터 집합에 여러 수식을 적용하여 값을 변화시킨다. 분포 모양이 달라진다
- 이 분포들을 **통계량 분포**라고도 부른다.

### 스튜던트 t분포
- `펫테일 fat tail` : 정규분포의 형태와 비슷하지만 양 끝의 꼬리부분이 더 두터운 모양
    - 현실에서는 극단적인 상황들이 발생할 수 있다. 이러한 극단적 상황의 데이터들이 반영된 형태
- 주식시장의 수익률은 일반적으로 정규분포를 따른다고 본다.
    - 블랙스완 : 자주 발생할 수 없는 극단적 현상을 금융계에서 일컫는 말
- 과거 주가 데이터 통해서 이러한 현상을 확인 할 수 있다.
- `스튜던트 t분포 student-t distribution` : 팻테일을 보이는 데이터 모형에 적합한 분포이다. t분포라고도 부른다.
    - pdf : $t(x;\mu,\lambda,\nu) = \dfrac{\sqrt{\lambda}}{\sqrt{\nu\pi}} \dfrac{\Gamma(\dfrac{\nu+1}{2})}{\Gamma(\dfrac{\nu}{2})} \left( 1 + \lambda\dfrac{(x-\mu)^2}{\nu} \right)^{-\dfrac{\nu+1}{2}}$ 
- 모수
    - mu : 1이 될 확률
    - lambda : 정규분포의 정밀도 (분산의 역수, beta, precision)
    - Gamma : 감마함수, 특수함수 $\Gamma(x) = \int_{0}^{\infty} u^{x-1} e^{-u} du$
    - nu : 자유도
- 스튜던트 t분포에서 nu는 자유도 값을 의미한다. 
    - **코시 분포 cauchy dist-** : 자유도가 1인 t분포
    - **하프 코시 분포 half cauchy dist-** : 코시 분포에서 양수인 부분만 사용하는 것
    - 자유도가 작아질 수록 : 분산이 커지고, 높이가 낮아지고, 꼬리가 두터워진다.
    - 자유도가 커질 수록 : 정규분포에 수렴한다.
- 스튜던트 t분포의 모멘트
    - 기댓값 : $\text{E}[X] = \mu$
    - 분산값 : $\text{Var}[X] = \dfrac{\nu}{\lambda(\nu-2)}$ (\nu > 2일때만 성립, \nu=1, 2이면 분산이 무한대가 된다.)
    
### t통계량
- `z 통계량` : 정규분포의 표본을 표준편차로 나눠서 정규화한 것, z통계량은 N의 증감에 상관없이 항상 정규분포이다.
    - z 통계량을 구하려면 표준편차를 알아야하는데 현실적으로 표준편차를 구하기 어렵다. 
    - 따라서 표본으로부터 표본표준편차로 정규화 할 수 밖에 없다.
- `t 통계량` : 정규분포로부터 얻은 N개의 표본 x1, x2, ..., xn 에서 계산한 표본평균을 표본표준편차로 정규화한 값.
    - t 통계량은 자유도가 N - 1인 스튜던트 t분포를 따른다.
    - $t = \dfrac{\bar{x}-\mu}{\dfrac{s}{\sqrt{N}}} \sim t \; (x;0, 1, N-1)$
    - xbar와 s는 각각 표본평균, 표본표준편차(비편향)이다.
- 시뮬레이션을 사용하여 t 통계량 분포를 그려보면 N이 작으면 스튜던트 t분포의 형태이고, N이 커지면 정규분포와 비슷해진다.

### 카이제곱분포
- t 통계량 : 확률변수 X로부터 얻은 N개의 표본 x1, x2, ..., xn의 표본평균 또는 표본의 합을 표본분산으로 정규화하면 스튜던트t분포를 따른다.
    - 모수 nu가 N-1 인 스튜던트t분포가 된다.
- `카이제곱분포 chi-squared dist-` : 확률변수 X에서 얻은 N개의 표본 x1~xn을 제곱합하면 양수값만 가지는 카이제곱분포가 된다. 
    - $x_i \sim \mathcal{N}(x;\mu, \sigma^2) \rightarrow \sum_{i=1}^{N} x_{i}^2 \sim \chi^2(x;\nu=N)$
    - 표본 xi가 정규분포를 따를때 표본들의 제곱합은 카이제곱분포를 따른다.
    - nu=N 은 표본의 갯수
- `카이제곱분포의 확률밀도함수 pdf`
    - $\dfrac{x^{(\dfrac{\nu}{2}-1)} \exp^{\left(-\dfrac{x}{2}\right)}} {2^{\left(\dfrac{\nu}{2}\right)} \cdot \Gamma\left(\dfrac{\nu}{2}\right)}$
    - 감마함수 : $\Gamma(x) = \dfrac{\nu}{\lambda(\nu-2)}$
- 카이제곱분포의 확률밀도함수에 따라서 N=nu 가 커지면 분모가 커지기때문에 0근처의 값들이 많이 나올 것 같다. 그러나 N이 2보다 커지면 0보다 조금 큰 1-2 근처의 값이 많이 나온다. 

### F분포
- `F분포` : 카이제곱분포를 따르는 독립적인 두 확률변수의 확률변수 표본을 각각 x1, x2라고 할때 이를 각각 N1, N2로 나눈뒤 비율을 구한 값.
    - $x_1 \sim \chi^2(N_1),\;\; x_2 \sim \chi^2(N_2) \rightarrow \dfrac{\dfrac{x_1}{N_1}}{\dfrac{x_2}{N_2}} \sim \text{F}(x;N_1, N_2)$
    - 스튜던트 t분포와 카이제곱분포는 정규분포를 따르는 확률변수 X로부터 나온 N개의 표본으로부터 만들 수 있었다. F분포도 같다.
- F분포의 확률밀도함수
     - $f(x;N_1, N_2) = \dfrac{\sqrt{\dfrac{(N_1x)^{N_1} \; N_2^{N_2}}{(N_1x + N_2)^{N_1 + N_2}}}} {x\;\text{B} \left( \dfrac{N_1}{2}, \dfrac{N_2}{2} \right)}$ 
     - B(x) : 베타함수
- 스튜던트 t분포의 표본도 제곱을 하면 F분포가 될 수 있다. (정규분포의 표본을 제곱합을 하면 카이제곱분포가 된다.)
    - t(N)^2 = F(1, N)
- F분포의 특징은 N1과 N2의 값이 같을 경우 값이 1근처의 값이 가장 많이 발생할 것 같지만 1이 아닌 다른 수가 더 흔하게 발생한다. 
    - (x1/N1) / (x2/N2) : N1=N2이면 x1/x2가 되므로 1이라는 직관적 계산이 가능하다.
    - 그러나 N1=N2인 시뮬레이션을 통해 발생하는 값의 분포를 살펴보면 1이 아닌 다른 값에서 더 많이 나온다.
    - 또한 N1=N2=a에서 a의 값이 커지면 오히려 0근처의 값이 많이 나온다는 것을 알 수 있다.
    
### 선형회귀분석에서의 확률분포의 활용
- 스튜던트 t분포, 카이제곱분포, F분포는 정규분포의 `통계량분포 statistics distribution`의 일종이다.
- 선형회귀 분석에서 각 분포들이 활용된다.
    - **스튜던트 t분포 : 추정된 가중치에 대한 확률분포**
    - **카이제곱분포 : 오차 제곱합에 대한 확률분포**
    - **F분포 : 비교 대상이 되는 선형모형의 오차 제곱합에 대한 비율의 확률분포**
- 정규분포를 따르는 확률변수 X에서 뽑은 N개의 표본집합 {xi}에 대하여
-    $X \rightarrow {x_i} = \left\{\begin{matrix}
     \sum x_i \rightarrow normal\;distribution \\
     \dfrac{\bar{x}-\mu}{\dfrac{\sigma}{\sqrt{N}}} \rightarrow z\;statistics \\
     \dfrac{\bar{x}-\mu}{\dfrac{s}{\sqrt{N}}} \rightarrow t\;statistics \\
     \sum_{i=1}^{N} x_{i}^2 \rightarrow chi^2\;distribution \\
     \dfrac{\sum_{i=1}^{N_1}x_{1}^2}{\sum_{i=1}^{N_2}x_{2}^2} \rightarrow F\;distribution \\
    \end{matrix}\right.$

# 다변수 정규분포

### 다변수 정규분포
- D차원의 `다변수정규분포 MVN, multivariate gaussian normal distribution`의 확률밀도함수
- 다변수정규분포의 확률밀도함수는 벡터를 입력받아서 이 벡터가 나올 확률=스칼라 값을 출력해준다.
    - $\mathcal{N}(x;\mu,\sum) = \dfrac{1}{\left(2\pi\right)^{D/2}|\sum|^{1/2}} \exp\left( -\dfrac{1}{2}(x-\mu)^{T} \cdot \sum^{-1} \cdot (x-\mu) \right)$
    - $x \in \text{R}^D$
    - $\mu \in \text{R}^D$, 평균벡터
    - $\sum \in \text{R}^{DxD}$, 공분산행렬
    - 괄호안의 수식은 x^TAx 인 이차형식이다. 즉 스칼라 값이 된다.
    - 지수함수 값도 상수가 된다. 
- 다변수정규분포에서 공분산행렬은 양의 정부호인 대칭행렬이어야 한다. 따라서 항상 역행렬이 존재한다.
    - **공분산행렬의 역행렬 : 정밀도 행렬 precision matrix**
- 증명 : `분산행렬의 부호가 양의 정부호이면 항상 역행렬이 존재한다.` (207p)
    - 분산행렬은 대칭행렬이다.
    - 대칭행렬이 양의 정부호이면 이 행렬의 고윳값이 모두 양의 정부호이다. (대칭행렬의 부호 성질 205p)
    - 역행렬이 존재하려면 행렬식의 값이 0이 아니어야 한다.
    - 행렬식의 성질에 의해 행렬식값은 모든 고윳값의 곱과 같다.
    - 이 대칭행렬의 행렬식값은 모든 고윳값의 곱과 같고, 모든 고윳값이 양의 정부호이므로 행렬식의 값도 0보다 크다.
    - 따라서 행렬식이 0보다 크므로 항상 역행렬을 갖는다.
- 2차원 다변수 정규분포 일때 : x = [x1, x2], mu = [2, 3], 공분산행렬 = [[1, 0], [0, 1]] (항등행렬) 이라고 가정
    - 확률밀도함수에 대입하여 정리하면 다음과 같다.
    - $\mathcal{N}(x_1, x_2) = \dfrac{1}{2\pi} \exp \left(- \dfrac{1}{2} ((x_1-2)^2 + (x_2-3)^2)\right)$
    - 지수함수의 거듭제곱이 + 인경우 두개의 지수함수로 나눌 수 있다. (분리가능 함수) : 
        - $\text{e}^{(x_1-2)^{2} + (x_2-3)^{2}} \rightarrow \text{e}^{(x_1-2)^2} \text{e}^{(x_2-3)^2}$
    - 이렇게 분리하면 **결합확률밀도함수 = 주변확률밀도함수의 곱**의 형태가 된다. 이런경우 **두 확률변수는 독립이다.** (독립과 상관에서 나옴)
        - p(x, y) = p(x1)p(x2)
    - 따라서 원형의 분포나, 각각의 축에 평행한 분포형태가 된다. (기울어지지 않음)
    - 이 조건에서는 원형의 분포가 나온다.
- 2차원 다변수 정규분포 일때 : x = [x1, x2], mu = [2, 3], 공분산행렬 = [[2, 3], [3, 7]] 이라고 가정
    - 두 확률변수의 **상관관계 공식**에 의해 공분산행렬의 원소를 대입해 계산하면 **양의 상관관계**라는 것을 알 수 있다.
    - 확률밀도 함수에 대입하여 정리하면 다음과 같다.
    - $\mathcal{N}(x_1, x_2) = \dfrac{1}{2\sqrt{5}\pi} \exp \left( \dfrac{7}{5}(x_1-2)^2 - \dfrac{6}{5}(x_1-2)(x_2-3) + \dfrac{2}{5}(x_2-3)^2 \right)$
    - 이 식에서는 지수함수의 곱으로 분리할 수 없다. - 가 있기 때문이다.
    - 이 조건에서는 타원의 분포가 나온다.
    - 조건부확률분포로 단면의 분포를 나타내보면 독립이 아니라는 것을 알 수 있다. 
    
### 다변수정규분포와 고윳값 분해
- 다변수정규분포의 공분산행렬은 양의 정부호인 대칭행렬이다. 따라서 **대각화 가능 diagonalizable** 이다.
    - 양의 정부호인 대칭행렬이면 고윳값이 모두 양수이다.
    - 고윳값이 모두 양수이면 대각화 가능하다. 
    - 대칭행렬을 대각화 할 수 있다. $\sum = V \Lambda V^T$
    - 또한 양의 정부호이면 역행렬이 존재하기 때문에 대각화를 역행렬로 만들 수 있다. $\sum^{-1} = V \Lambda^{-1} V^T$
- 공분산행렬의 역행렬(=정밀도 행렬)의 고유분해
    - $\sum^{-1} = V \Lambda V^{T}$, ($\sum^{-1} = (V \Lambda V^{-1})^{-1} = V \Lambda^{-1} V^{-1} = V \Lambda^{-1} V^{T}$)
- 정밀도 행렬의 고유분해를 다변수정규분포의 확률밀도함수에 대입하면,
    - $\mathcal{N} \propto \exp \left( \dfrac{1}{2} (x-\mu)^{T} \sum^{-1} (x-\mu) \right)$
    >- $= \exp \left( \dfrac{1}{2} (x-\mu)^{T} V \Lambda^{-1} V^{T} (x-\mu) \right)$
    >- $= \exp \left( \dfrac{1}{2} (V^{T}(x-\mu))^{T} \Lambda^{-1} (V^{T} (x-\mu)) \right)$ 
    >- $= \exp \left( \dfrac{1}{2} (V^{-1}(x-\mu))^{T} \Lambda^{-1} (V^{-1} (x-\mu)) \right)$, ((sol : V^{T} => V^{-1}}))
- 정밀도 행렬의 고유분해값을 대입하여 식을 정리하면, **확률밀도함수의 좌표변환을 의미한다.**
    - $x^{'} = V^{-1}(x-\mu)$
    - x를 mu만큼 평행이동 한다. 이 값에 변환행렬을 곱해주는 형태. 
        - 고유벡터행렬은 고유벡터를 열벡터로 가지는 행렬인데, 이러한 행렬의 역행렬을 **변환행렬** 이라고 한다. (좌표변환에 필요한 행렬)
        - 변환행렬의 열벡터는 새로운 좌표의 베이시스 벡터이다.
        - 즉 V^-1의 열벡터인 고유벡터들이 새로운 베이시스 벡터가 되고 타원의 축이 된다. 
    - 따라서 x를 mu만큼 이동한 후 V^-1 로 좌표변환 한다는 의미이다. 
- 따라서 확률밀도함수는 원래좌표에서 고유벡터 방향으로 회전시킨 모양이다.
    - $\mathcal{N} \propto \exp \left(- \dfrac{1}{2} x^{'T} \Lambda^{-1} x^{'} \right)$
    - 확률밀도함수는 \mu가 중심이고 반지름은 고윳값에 비례하는 타원이다.
- 즉, 다변수정규분포의 정밀도행렬을 고유분해하여 식에 대입하면 좌표변환을 하는 것과 같아진다.
    - 공분산행렬의 고윳값이 타원의 가로와 세로 폭이 된다. 
    - 공분산행렬의 고유벡터가 타원의 방향이 된다. 
- 다변수정규분포의 공분산행렬이 고윳값의 비율 = "컨디션넘버" (회귀분석에서 나오는 개념)
    - 컨디션넘버가 클 수록 다중공선성이 강해진다. 즉 확률변수간 상관관계가 커진다. 
    - 컨디션넘버가 커지는 경우는 원래 다변수정규분포가 상관관계가 있는 경우와 이것을 좌표변환(또는 스케일링)했을 때 고윳값의 단위차이가 날 경우이다.
    
### 다변수정규분포의 조건부확률분포
- `다변수정규분포인 확률변수벡터 중 어떤 원소의 값이 주어지면 다른 확률변수의 조건부확률분포는 다변수정규분포이다.`
    - 즉 다변수 정규분포의 확률밀도함수를 자른 단면(x1, x2 중 하나의 값을 고정시킨 후 자름)의 분포도 정규분포라는 의미이다.
- x2를 임의의 값으로 고정시킨 후 자른 단면의 분포의 중심값은(\mu_{1|2}) 원래 다변수정규분포의 중심값(\mu_{1})과 차이가 생긴다. 
    - $\mu_{1|2} = \mu_1 - \Lambda_{11}^{-1} \Lambda_{12} (x_2 - \mu_2)$
    
### 다변수정규분포의 주변확률분포
- `다변수정규분포의 주변확률분포는 다변수정규분포이다.`
    - 즉 결합확률밀도함수를 어떤 확률변수의 값으로 적분하여 나머지 확률변수의 주변확률분포를 구하면 다변수정규분포라는 의미이다.
    - 확률밀도함수를 x1, x2 중 어떤 확률변수로든 조건부확률분포 즉 모든 단면의 면적(적분)을 구하면 이것의 분포가 정규분포를 따른다.

### 다변수정규분포의 의미
- 다변수정규분포의 조건부확률분포나 주변확률분포가 모두 정규분포를 따른다는 의미이다.

# 베타분포, 감마분포, 디리클레분포
- 베타분포, 감마분포, 디리클레 분포는 모숫값을 조정하여 분포의 모양을 조절 할 수 있다.
    - 모숫값 조절 -> 분포의 모양 조절
- 이 분포들은 데이터의 분포를 나타내기 보다 베이지안 확률론의 관점에서 확신 또는 신뢰의 정도를 나타내는데 주로 사용된다.

### 베타분포
- `베타분포 beta distribution` : 모수 a,b를 갖는다. 표본공간은 0과 1사이의 실수이다. 0과 1사이의 실수값만 표본으로 갖는다.
    - $\text{Beta}(x;a,b),\;\;\; 0 \leq x \leq 1$
- `베타분포의 pdf`
    - $\text{Beta}(x;a,b) = \dfrac{\Gamma(a + b)}{\Gamma(a) \Gamma(b)} x^{a-1} (1-x)^{b-1}$
    - $\Gamma(a) = \int_{0}^{\infty} x^{a-1} e^{-x} dx$, ((570p에도 나온다.))
- `베타분포의 모양`
    - sp.stats.beta(a, b)
    - a와 b의 값이 커질 수록 분포의 분산이 작아진다. 즉 폭이 좁아진다.
- `베타분포의 모멘트`
    - $\text{E}(x) = \dfrac{a}{a+b}$
    - $mode = \dfrac{a-1}{a+b-2}$, ((a와 b의 값이 같을때 식을 정리하면 0.5의 최빈값이 된다.))
    - $\text{Var}(x) = \dfrac{ab}{(a+b)^2(a+b+1)}$, ((분산의 식에 따라서 a, b의 값이 커지면 분모가 커진다는 것을 알 수 있다.))


### 감마분포
- `감마분포 gamma distribution` : 모수가 a, b이고, 어떤 모수의 베이지안 추정에 사용된다. 0부터 무한대의 양수값에 대한 추정에 사용된다.
    - $\text{Gam}(x;a,b) = \dfrac{1}{\Gamma(a)} b^{a} x^{a-1} e^{-bx}$
- `감마분포의 모멘트`
    - $\text{E}[X] = \dfrac{a}{b}$
    - $\text{mode} = \dfrac{a-1}{b}$
    - $\text{Var}[X] = \dfrac{a}{b^2}$
- `감마분포의 분포모양`
    - 베타분포와 마찬가지로 어떤 모수값에 대한 베이지안 확률론의 추정에 사용되므로, 분포의 분산의 크기가 모수값의 추정에 대한 신뢰도를 의미한다.
    - 사이파이의 stapts 서브패키지의 gamma() 클래스를 사용하여 만들 수 있다. 모수 b의 값이 1로 고정되어 있다.

### 디리클레분포
- `디리클레분포 dirichlet distribution` : 베타분포의 확장된 분포. 0과 1사이의 값을 가지는 다변수(multivariate) 확률변수의 베이지안 모형에 사용된다.
    - 3개의 변수가 사용된 경우 : (0.5, 0.6, 0.7) 또는 (1, 0, 0) 등의 0과 1사이의 값으로 이루어진 다변수를 입력받는다.
    - 베타분포는 0과 1사이의 단일(univariate) 확률변수의 베이지안 모형에 사용 됨 : (0.4) 또는 (0.145) 등 0과 1사이의 값으로 이루어진 단변수를 입력받는다.
- `디리클레분포의 확률밀도함수`
    - $\text{Dir}(x_1, x_2, \cdots, x_k ; \alpha_1, \alpha_2, \cdots, \alpha_k) = \dfrac{1}{\text{B}(\alpha_1, \alpha_2, \cdots, \alpha_k)} \prod_{i=1}^{K} x_{i}^{\alpha_i -1}$
    - 베타함수 : $B(\alpha_1, \alpha_2, \cdots, \alpha_k) = \dfrac{\prod_{i=1}^{K} \Gamma(\alpha_i)}{\Gamma\left( \sum_{i=1}^{K} \alpha_i\right)}$
    - 표본값 벡터 x = (x_1, x_2, ..., x_k), 모수값 벡터 \alpha = (\alpha_1, \alpha_2, ..., \alpha_k)
- `디리클레분포의 확률값 x의 제한조건`
    - $0 \leq x_i \leq 1$, $\sum_{i=1}^{K} x_i = 1$

### 베타분포와 디리클레분포의 관계
- **베타분포는 K=2 즉 입력변수가 2개인 디레클레분포와 같다.** x1 = x, x2 = 1-x, \alpha_1 = a, \alpha_2 = b 라고 한후, 베타분포의 확률밀도함수에 각각을 대입하여 정리한다.
- 디리클레분포의 확률밀도함수에 사용된 베타분포의 형태가 감마함수의 곱의 형태와 모수의 합을 입력받는 감마함수의 분수형태이므로, 위의 식의 형태와 같아진다.

$\begin{aligned}
    \text{Beta}(x;a,b)
    &= \dfrac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)} x^{a-1} (1-x)^{b-1} \\
    &= \dfrac{\Gamma(\alpha_1+\alpha_2)}{\Gamma(\alpha_1)\Gamma(\alpha_2)} x_{1}^{\alpha_1-1} x_{2}^{\alpha_2-1} \\
    &= \dfrac{1}{\text{B}(\alpha_1, \alpha_2)} \prod_{i=1}^{K} x_{i}^{\alpha_i-1}\\
\end{aligned}$

### 디리클레분포의 모멘트
- 기댓값 : $\text{E}[x_k] = \dfrac{\alpha_k}{\sum \alpha}$
- 최빈값 : $mode = \dfrac{\alpha_k - 1}{\sum \alpha - K}$
- 분산 : $\text{Var}[x_k] = \dfrac{\alpha_k \left( \sum \alpha - \alpha_k \right)}{\left( \sum \alpha \right)^{2} \left( \sum \alpha + 1 \right)}$
- **기댓값 공식에서의 모수 \alpha 벡터는 입력변수인 x_k 벡터 중에서 어떤 수가 나올 가능성이 더 큰지를 결정하는 형상인자 shape factor이다.** 모든 모수의 값이 같으면 모든 xi의 확률분포가 같아진다. 나올확률이 같다.
- **분산 공식에서 모수 벡터가 값이 클 수록 값이 작아진다.** 즉 디리클레분포의 표본값 x가 어떤 특정한 값 근처에서 나올 가능성이 높다는 의미이다.

### 디리클레분포의 응용
- `3개의 양의 난수 x, y, z가 주어지고 항상 x + y + z = 1이 되게 하려면 어떻게 해야할까? 모든 경우가 균등해야 한다.`
    - 이러한 문제는 K=3이고, 모수가 \alpha_1 = \alpha_2 = \alpha_3인 디리클레분포의 특수한 경우이다.
    - 또한 3차원 공간에서 (1, 0, 0), (0, 1, 0), (0, 0, 1) 세점을 연결하는 정삼각형 면이 있고, 이 면 위에 점을 생성하는 문제와 같다. 디리클레분포의 확률값 제한조건에 의해서 x,y,z는 각각 1보다 클 수 없다.
- 이 정삼각형 범위안에서 일반 난수를 생성하면 점의 분포가 고르지 않고 중앙에 집중 되지만, 디리클레분포에 모수를 균일한 값으로 설정하여 난수를 생성하면 정삼각형 범위안에서 고르게 생성된다.
- **sp.stats.dirichlet((a, b, c))** : (a, b, c) 는 모수벡터 즉 모수값 \alpha_1, \alpha_2, \alpha_3 으로 이루어진 벡터이다.

### 베이지안 추정
- K=3이고, 모수가 (1, 1, 1)이 아닌 경우 : 정삼각형 안에 특정한 위치에 분포가 집중되는 것과 같다.
    - 이러한 특성을 이용하면 카테고리분포의 모수벡터 \mu를 추정한 결과를 나타낼 수 있다.
    - 즉 디리클레분포에 모수벡터 (0.3, 0.5, 0.5)를 입력하면 정삼각형 안에 특정한 값주면에 분포가 넓게 생긴다.
    - 다시 디리블레분포에 모수벡터 (30, 50, 50)을 입력하면 같은 값주변에 분포가 좁게 생긴다. 특정값 주변에서 카테고리분포의 모수가 나올 가능성이 높아진 것과 같다.






