# 다시 이해하게 된 것들

## 1. ◎ 고유분해 eigen-decomposition 중에서
### 분산행렬 X@X.T의 특징
- 어떠한 형태의 행렬이라도 분산행렬로 만들면 정방-대칭행렬이 된다.
  - (3x2) @ (2x3) = (3x3)
- 대칭행렬의 이차형식으로 부호를 판단하면 역행렬이 존재하는지, 풀랭크인지, 열벡터들이 선형독립인지 판단할 수 있다.
- 즉 풀랭크인 데이터 행렬일 수록 좋은 데이터라고 할때, 데이터의 좋고 나쁨을 판단하는 기준이 될 수 있다. 
```
<어떤 행렬 X 가 풀랭크이면 이 행렬의 분산행렬의 역행렬은 항상 존재한다.>

행렬 X 가 풀랭크이면 분산행렬이 양의 정부호가 된다. 
분산행렬은 대칭행렬이므로 대칭행렬이 양의 정부호이면 고윳값이 모두 양수이다. 0은 존재하지 않는다.
이 대칭행렬이 행렬식은 모든 고윳값의 곱이므로 0이 아니다.
행렬식이 0이 아니므로 대칭행렬의 역행렬이 항상 존재한다.
```
```
<역행렬이 존재하는 대칭행렬은 항상 양의 정부호가 아니다.>

역행렬의 존재조건인 행렬식이 양수 또는 음수가 될 수 있다.
행렬식이 음수이면, 고윳값중에 한 개 이상이 음수이어야 한다.
대칭행렬과 고윳값의 부호정리에 의해서, 고윳값 중에 음수가 있다면 대칭행렬은 양의 정부호가 아니다.
그러므로 역행렬이 존재한다고 해서 항상 양의 정부호라고 할 수 없다.
```

### 고유분해의 성질 요약
- 데이터 분석에서 자주 사용됨
```
* 행렬 A는 N개의 고윳값-고유벡터를 가진다. (복소수인 경우와 중복인 경우를 포함)
* 행렬의 대각합은 모든 고윳값의 합과 같다.
* 행렬의 행렬식은 모든 고윳값의 곱과 같다.
* 행렬 A가 대칭행렬이면 실수 고윳값 N개를 가지며 고유벡터들이 서로 직교이다.
* 행렬 A가 대칭행렬이고 고윳값이 모두 양수이면 양의 정부호이고 역행렬이 존재한다. 역도 성립한다.
* 행렬 A가 어떤 행렬 X의 분산행렬이면 0 또는 양의 고윳값을 가진다.
* 행렬 X가 풀랭크이면 분산행렬은 역행렬이 존재한다.
```

## ◎ 근사문제
- 임의의 벡터들과 거리가 가장 가까운 직선이나, 벡터공간을 찾는 문제

### 조건
- N개의 M차원 벡터 a1, a2, ..., aN 과 거리가 가장 가까운 직선의 단위벡터 w를 찾는다.
- 또는 서로 직교하는 K개의 벡터 w1, w2, ..., wK가 기저벡터인 벡터공간 V에 N개의 M차원 벡터를 투영시켰을때 가장 유사한 투영벡터들을 만드는 w1, w2, ...,wk를 찾는다.

### 접근법
- 벡터 ai와 직선 w와의 거리를 작게하는 조건을 만들고 이 조건을 만족하는 값을 찾는방법
  - 1차원 근사문제, 일반적 근사문제, k차원 근사문제
- 벡터 ai를 직선 w나 벡터공간의 기저벡터들에 각각 투영해서 만든 투영벡터가 기존의 벡터들을 행벡터로 갖는 행렬 A와 가장 비슷한 행렬 A'를 찾는방법
  - 랭크-1 근사문제, 랭크-K 근사문제

### 요약
- 근사문제의 핵심은 임의의 벡터들과 가장 유사한 직선이나, 벡터공간을 찾는 것이다. 
- 이를 위해서 임의의 벡터들을 행렬로 만들고 벡터의 기하학적 표현원리들을 사용하여 직선이나 벡터공간과의 거리를 수식화 한다.
- 선형대수의 법칙들을 사용하여 임의의 벡터들과 직선 또는 벡터공간과의 거리를 최소화하거나, 투영벡터가 원래의 벡터와 가장 유사하게 만든다.
- 이러한 과정에 의해 임의의 벡터들을 행벡터로 갖는 행렬 A의 특이분해에 의해 생성된 가장 큰 K개의 특잇값과 이에 대응하는 오르쪽 특이벡터 K가 값이 된다는 것을 알 수 있다.
- 근사문제의 원리를 사용하여 PCA(주성분 분석) 분석을 할 수 있다.
  - 주성분 분석을 통하여 붓꽃의 크기, 주식 가격 변화, 집 값 변화, 선수 연봉변화 등의 여러가지 선형적 데이터의 변화 규칙을 찾아 예측할 수 있게 된다.

## 2. 추천시스템에 사용되는 특이분해의 원리
- 추천시스템에서 두개의 행렬로 나눌때 특이분해의 축소형 모델이 사용됨
- <img src="https://latex.codecogs.com/png.latex?%5Cdpi%7B120%7D%20%5Cfn_cm%20%5Clarge%20U%5CSigma%5E%5Cfrac%7B1%7D%7B2%7D%5CSigma%5E%5Cfrac%7B1%7D%7B2%7DV%5ET">
- 왼쪽특이벡터행렬, 특잇값 행렬, 오른쪽특이벡터행렬

## 3. 그레디언트벡터, 자코비안행렬, 헤시안행렬
### 그레디언트벡터
- 벡터를 출력하는 함수를 스칼라 입력변수로 행렬미분해서 얻은 벡터
- `기울기(1차 도함수)로 이루어진 벡터`
- 벡터의 길이는 입력변수의 수와 같다.

#### 그레디언트벡터 특징
- contour 플롯(등고선)에 quiver 플롯(화살표)으로 그려서 나타낼 수 있다.
- 즉 그레디언트 벡터는 경사의 급한 정도와 경사가 급한 방향을 가리키는 화살표와 같다.
- 화살표의 길이는 경사의 크기, 길이가 길면 경사가 큼, 길이가 짧으면 경사가 작음
- 화살표의 방향은 단위길이당 함수값의 변화가 큰 방향
- 등고선과 그레디언트 벡터의 방향은 직교한다. : 테일러전개로 증명이 된다.

### 자코비안행렬
- 벡터를 출력하는 함수를 벡터 입력변수로 행렬미분해서 얻은 행렬
- 벡터함수를 벡터로 미분해서 얻은 행렬의 전치행렬이 자코비안 행렬과 같다.
- `기울기(1차 도함수)로 이루어진 행렬`

### 헤시안행렬
- 2차 도함수로 이루어진 행렬
- 그레디언트벡터를 다시 벡터로 미분해서 얻은 행렬
- 그레디언트벡터의 자코비안행렬의 전치행렬과 같다. 
  - 자코비안행렬 : 벡터를 벡터로 미분해서 얻은 행렬
- 구하는 방법
  - 주어진 함수의 그레디언트벡터를 구한다. (벡터를 스칼라로 미분한다)
  - 그레디언트벡터의 자코비안행렬을 구한다. (벡터를 벡터로 미분한다. 전치 한번한다)
  - 구한 행렬을 전치연산한다.
- 함수가 연속이고, 미분가능하다면 헤시안행렬은 대칭행렬이 된다. (정방행렬이다, 고유분해를 할 수 있다, 고유분해의 특징을 따른다)

## 4. 최적화와 최적제어

### 최적화 optimization
- 최적화 : 목적함수 f 의 값을 최대 또는 최소가 되도록 하는 독립변수 x 를 찾는 방법
  - 최적화의 대상은 목적함수
  - 목적함수 :
    - 성능함수 performance function (모수의 변화에 함수의 성능을 측정함, 클 수록 좋음)
    - 손실함수 loss function (모수의 변화에 따라 달라지는 출력값의 오차를 측정, 작을 수록 좋음), 비용함수, 오차함수라고도 함
- 최적화를 위한 필요조건 : 최적의 독립변수 `x*`을 입력했을 때 함수의 도함수 값이 0이 되어야한다. 
  - 1차 도함수의 값이 0 인 지점이 최대 또는 최소값 지점이다.
  - 2차 도함수의 값이 - 이면 최대값, + 이면 최소값이다. (간단한 그래프로 확인가능 3차함수-2차함수-1차함수)
  
### 최적제어 optimal control
- 최적제어 : 범함수 F 의 값을 최대 또는 최소가 되도록 하는 독립함수 y(x) 를 찾는 방법
- 최적제어의 필요조건 : 최적의 함수 `y*(x)` 을 입력했을 때 범함수의 도함수 값이 0이 되어야 한다. 
- 딥러닝의 GAN 모형 : 현실 데이터와 닮은 데이터를 재현하는데 사용하는 방법으로 다음 범함수의 값을 최대화하는 확률분포함수 p(x) 를 구해준다.
- <img src="https://latex.codecogs.com/gif.latex?%5Cdpi%7B100%7D%20L%5Bp%5D%20%3D%20%5Cdfrac%7B1%7D%7B2%7D%20%5Cint%20%28%5Clog%20%28p%28x%29p_%7Bdata%7D%28x%29%20&plus;%20%5Clog%20%281-p%28x%29%29p_%7Bmodel%7D%28x%29%29%20dx"/>

- 이 범함수의 도함수
> <img src="https://latex.codecogs.com/gif.latex?%5Cdfrac%7B%5Cdelta%20L%7D%7B%5Cdelta%20p%7D%20%3D%20%5Cdfrac%7B1%7D%7B2%7D%20%5Cdfrac%7B%5Cpartial%7D%7B%5Cpartial%20p%7D%20%5Clog%20p%28x%29p_%7Bdata%7D%28x%29%20&plus;%20%5Cdfrac%7B1%7D%7B2%7D%20%5Cdfrac%7B%5Cpartial%7D%7B%5Cpartial%20p%7D%20%5Clog%20%281-p%28x%29%29p_%7Bmodel%7D%28x%29"/>

> - 위의 식에서 로그미분과 결합법칙을 사용하여 정리하면
> <img src="https://latex.codecogs.com/gif.latex?%5Cdfrac%7Bp_%7Bdata%7D%28x%29%281-p%28x%29%29%20-%20p%28x%29p_%7Bmodel%7D%28x%29%7D%7B2p%28x%29%281-p%28x%29%29%7D">

> - 이 도함수의 값을 0으로 만드는 최적 확률분포함수 p(x) 는
> <img src="https://latex.codecogs.com/gif.latex?%5Cdpi%7B100%7D%20p_%7Bdata%7D%28x%29%281-p%28x%29%29%20-%20p%28x%29p_%7Bmodel%7D%28x%29%3D0"/>

- 수식을 정리하면,
> <img src="https://latex.codecogs.com/gif.latex?p%5E*%28x%29%20%3D%20%5Cdfrac%7Bp_%7Bdata%7D%28x%29%7D%7Bp_%7Bdata%7D%28x%29%20&plus;%20p_%7Bmodel%7D%28x%29%7D"/>

## 5. 확률변수의 확률분포함수를 설명하는 대표값
- 현실 데이터와 확률변수의 관계에서 발생하는 기술통계값의 의미
    - $\text{E}[X], \text{E}[\bar{X}], \text{Var}[X], \text{Var}[\bar{X}], S^{2}, \text{E}[S^2]$
- 확률분포함수의 모양을 설명하는 기댓값과 분산을 계산하기 위한 과정의 의미 

### 확률변수의 확률분포의 의미
- 1) 현실의 데이터는 모두 확률변수에서 생성된 데이터 집합이다.
    - 데이터셋에서 각각의 독립변수는 각각의 확률변수로 부터 생성
- 2) 확률변수의 확률분포함수를 알면 데이터 분석의 필요한 값들을 계산할 수 있다.
    - 확률분포함수는 확률질량함수(pmf), 누적분포함수(cdf), 확률밀도함수(pdf) 등이 있다.
    - 데이터셋은 불완전하므로 확률변수를 통해서 정확한 값들을 알아 낼 수 있다.
- 3) 하지만 확률변수를 알기 위해서 데이터셋으로부터 역설계하여 유추해야 한다.
- 4) 확률변수의 확률분포함수를 알기 위해서는 분포의 모양을 표현하는 대표값들이 필요하다.
    - 기댓값(평균) : 분포의 위치
    - 분산 : 분포의 폭, 신뢰도
- 5) 대표값들을 계산하여 확률분포를 알게 되면 데이터셋의 특징을 수치화할 수 있게된다.
- 6) 데이터셋으로부터 확률변수를 유추하는 과정에서 여러가지 오차들이 발생하게 된다.
    - 이 오차들을 보완하는 과정에서 선형대수의 기술통계값들에 대해서 이해할 수 있게 된다.

### 확률분포의 기댓값
- 1) $\text{E}[X]$ : 기댓값은 범함수이다. 범함수는 함수를 입력받는 함수이다.
    - 실수를 입력받는 함수는 일반적인 함수이다.
- 2) 확률분포의 모양을 설명하는 대표값으로 기댓값은 확률분포의 위치를 나타낸다.
    - 즉 확률분포가 어떤 구간(지점)에서 확률이 높은지를 알 수 있으므로 기댓값은 가장 확률이 높은 구간(지점)을 의미한다.
- 3) 그런데 확률변수 X의 기댓값은 구하기 어렵다. 따라서 표본평균을 구하여 기댓값을 알 수 있다.
    - 확률변수를 데이터셋으로부터 유추해야 하므로 알지 못한다.
    - 표본평균 즉 데이터셋의 평균도 표본평균확률변수로 부터 나왔다고 할 수 있다.
- 4) 표본평균 확률변수 $\bar{X}$는 확률변수 X의 복사본인 X1-Xn의 확률변수의 평균을 모아놓은 것과 같다.
    - 표본평균 확률변수의 기댓값은 원래 확률변수의 기댓값과 같다.
    - $\text{E}[\bar{x}] = \text{E}[X]$
- 5) 즉 데이터셋의 표본평균은 표본평균 확률변수에서 나온 것이고 이 값의 기댓값이 원래 확률변수 X의 기댓값과 같다.
- 6) 표본평균 확률변수의 분산을 구하여 신뢰도를 알 수 있다.
    - 얼마나 정확한 값인지 신뢰도를 분산을 통해 알 수 있다.
- 7) 표본평균 확률변수의 분산은 원래 확률변수의 분산보다 1/N 만큼 작다.
    - 분산이 작다는 것은 신뢰도가 높다는 것을 의미한다.
    - 어떤 분포의 분산이 작을 수록 한 곳으로 몰리기 때문
    - $\text{Var}[\bar{X}] = \dfrac{1}{N} \text{Var}[X]$
- 8) 데이터의 갯수(N)가 무한대로 커질 수록 표본평균 확률변수의 분산은 0에 가까워지고 기댓값은 결정론적 값이 된다.
    - 표본평균 확률변수의 값이 고정된다는 의미
- 9) 따라서 데이터셋의 평균을 표본평균 확률변수의 기댓값과 같고, 표본평균 $\bar{x}$는 확률변수 X의 기댓값과 거의 같아진다.
    - 데이터가 무한하게 많지 않으므로 표본평균은 기댓값의 근처에서 나오게 된다.

### 확률분포의 분산
- 1) 데이터셋은 불완전하므로 분포가 한쪽으로 쏠리게 된다.
- 2) 따라서 표본평균이 편향된 데이터 쪽으로 치우치게 된다.
    - 편향의 정도가 크진 않고 확률변수의 기댓값 근처이다.
- 3) 표본평균이 편향되면 그 만큼 표본분산이 작아진다.
    - 표본분산은 데이터와 평균의 거리이므로 이 거리가 작아진다.
    - 표본평균과 마찬가지로 표본분산도 편향된다.
- 4) 이론적인 분산인 $\sigma^2 = \text{Var}[X]$은 기댓값이 정확하므로 이 값과 같은 비편향 표본분산을 구한다.
    - $\sigma^2 = \text{E}[(X-\mu)^2]$
- 5) 비편향 표본분산은 표본분산의 기댓값으로부터 구할 수 있다.
    - $\text{E}[S^2] = \dfrac{N-1}{N} \sigma^2$
    - 이론적인 분산보다 작아지게 된다.
- 6) 이 식으로부터 비편향 표본분산이 유도 된다.
    - $\sigma^2 = \dfrac{N}{N-1} \text{E}[S^2]$
- 7) 즉 이론적 분산과 같은 표본분산은 비편향 표본분산이고 표본분산을 N/N-1 만큼 확대한 값과 같다.
    - 데이터가 10개이면 10/9 즉 1.1배 표본분산을 확대해야 이론적 분산과 같아진다는 의미이다.





