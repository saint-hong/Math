# Advanced Linear Algebra

### 1. 선형대수와 해석기하

#### 벡터의 기하학적 의미
- 벡터 a 는 n 차원 공간에서 점 또는 원점에서 점까지의 화살표를 의미한다.
- n 차원 공간에서의 벡터 a 는 평행이동 할 수 있다.
- 벡터의 길이는 놈 norm 으로 정의한다.
    - ```벡터의 길이``` : $\|a\|=\sqrt{a^Ta}=\sqrt{a_{1}^2+a_{2}^2+a_{3}^2+...+a_{N}^2}$
    - 원래 놈의 제곱은 이런 형태이다. $\|a\|^2=a^Ta=\sum_{i=1}^N\sum_{j=1}^Na_{i,j}^2$

#### 스칼라와 벡터의 곱
- 양의 실수 c 와 벡터 a 를 곱하면 방향은 고정, 길이가 실수 크기만큼 커진다.
- 음의 실수 c 와 벡터 a 를 곱하면 방향은 반대가 되고, 길이가 실수 크기만큼 커진다.

#### 단위벡터 unit vector
- ```길이가 1 인 벡터를 단위벡터라고 한다.``` (놈의 정의에 의해서 원소들의 제곱합이 1 인 벡터들은 모두 단위벡터 이다.)
- 영벡터가 아닌 어떤 벡터 x 에 대하여 x 의 놈, 즉 길이로 나눈 것은 x 의 단위벡터가 된다. : $\dfrac x{\|x\|}$

#### 벡터의 합
- 두 벡터의 합은 두 벡터를 이웃하는 변으로 가지는 평행사변형의 대각선 벡터가 된다.

#### 벡터의 선형조합
- 여러개의 벡터에 스칼라곱을 한 후 모두 더한 것을 ```선형조합 linear combination``` 이라고 한다.
- n 차원 공간에서 벡터의 선형조합의 의미는, 각 벡터에 가중치가 곱해져 길이나 방향이 변하고, 변한 벡터들을 더한 위치의 벡터가 된다.

#### 벡터의 차
- a-b=c 는 벡터 b 가 가리키는 점으로부터 벡터 a 가 가리키는 점을 연결하는 벡터다.
- b + c = b + (a - b) = a

#### wrod2Vec (인공신경망)
- 벡터의 차를 활용한 단어의미 분석
- 단어의 의미를 벡터로 표현하고, 벡터의 평행이동을 적용하여 다른 단어에도 적용함으로써 같은 의미를 찾을 수 있다.
    - ```한국 - 서울```은 ```서울->한국```으로 향하는 벡터일 때, 이것은 수도이름을 나라이름으로 바꾸는 행위와 같다.
    - 이러한 행위를 파리에 적용하면 ```파리 + (한국-서울)``` 이라고 벡터로 표시 할 수 있다.
    - 이러한 벡터의 연산을 word2Vec 에 학습 시키면, 파리와 가장 가까이에 위치한 단어인 ```프랑스```가 나온다.

#### 유클리드 거리
- ```유클리드 거리 Euclidean distance``` : 두 벡터가 가리키는 점 사이의 거리
- 벡터의 차의 길이로 구한다.
- $\|a-b\| = \sqrt{\sum_{i=1}(a_i-b_i)^2}=\sqrt{\sum_{i=1} ( a_i^2 - 2 a_i b_i + b_i^2 )}=\sqrt{\sum_{i=1} a_i^2 + \sum_{i=1} b_i^2 - 2 \sum_{i=1} a_i b_i}= \sqrt{\| a \|^2 + \| b \|^2  - 2 a^Tb }$
- $\|a-b\|^2 = \|a\|^2+\|b\|^2-2a^Tb$

#### 벡터의 내적과 삼각함수
- 두 벡터의 내적은 벡터의 길이와 벡터사이의 각도의 ```코사인 함숫값```으로 계산할 수 있다.
    - $a^Tb=\|a\|\|b\|cos\theta$
    - 삼각함수 : $sin\theta$, $cos\theta$, $tan\theta$
    - 빗변과 높이의 비율 : $sin\theta=\dfrac{a}{h}$, 빗변과 밑변의 비율 : $cos\theta=\dfrac{b}{h}$ (따라서 각도에 따라 반대의 성질을 갖는다.)
    - 두 벡터의 각도가 90도 일때 : $sin\theta=1$, $cos\theta=0$
    - 두 벡터의 각도가 0도 일때 (방향이 완전히 같을때) : $sin\theta=0$, $cos\theta=1$

#### 직교
- ```직교 orthgonal``` : 두 벡터 사이의 각도가 90도 일때 직교한다고 정의한다.
- 각도가 90도 이면 $cos\theta=0$ 이므로, 두 벡터의 내적은 공식에 적용하면, 0 이 된다.
    - $a^Tb = b^Ta = 0 \;\;\; \leftrightarrow \;\;\; a \perp b$
    - 내적값이 0 이면 직교한다고 할 수 있다.
- 행렬은 기본적으로 각 열벡터가 기저를 이루는 좌표계 cordinate system 이다.
    - 직교행렬 : 행렬의 모든 열벡터들이 서로 직교
    - 정규직교행렬 : 행렬이 직교행렬이고 모든 열벡터의 크기가 1이면 정규직교
- 어떤 행렬이 직교행렬인지 아닌지 확인하려면, 열벡터들의 내적이 0인지 아닌지 확인하면 된다.

#### 정규직교
- ```정규직교 orthonormal``` : ```N 개의 단위벡터``` v1,v2,v3,...vN 이 서로 직교하면 정규직교라고 한다.
    - $\|v_{i}\| = 1 \;\; \leftrightarrow \;\; v_{i}^Tv_{j} = 1$
    - $v_{i}^Tv{j} = 0 \;\; (i \neq j) $, $v_{i}^Tv_{j} = 1 \;\; (i = j)$
    - $V^TV=I$, $V^{-1}=V^{T}$

#### 코사인 유사도
- ```코사인 유사도 cosine similarity``` : 두 벡터의 방향이 비슷할 수록 벡터가 비슷하다고 간주하여, 두 벡터 사이의 각의 코사인 값을 말한다.
- 각도가 0 일때 코사인값이 가장 커지므로, 두 벡터가 같은 방향을 가리키면 코사인 유사도가 최댓값 1을 가진다.
    - $\text코사인 유사도 = \cos\theta=\dfrac{x^Ty}{\|x\|\|y\|}$ (벡터의 내적 공식에서 도출)
- ```코사인 거리 cosine distance``` : 코사인 유사도를 사용하여 두 벡터간의 거리를 측정한다. ```추천시스템 recommender system``` 에서 사용된다.
    - $\text코사인 거리 = 1-코사인 유사도 = 1-\dfrac{x^Ty}{\|x\|\|y\|}$
    - 분모는 벡터의 길이간의 곱이므로 내적이 아니다.

#### 벡터의 성분과 분해
- 벡터 a 와 b 의 합으로 벡터 c 를 만들 수 있다. 이때 벡터 c 는 ```성분 component``` a, b 로 ```분해 decomposition``` 된다고 한다.

#### 벡터의 투영성분과 직교성분
- 벡터 a 는 벡터 b 에 대한 ```투영성분 projection``` 과 ```직교성분 rejection``` 으로 분해 된다.
- 벡터 a 에 수직으로 햇빛이 내리쬐면 바닥에 생기는 그림자를 투영성분, 벡터 a 에서 바닥으로 그은 직선을 직교성분이라고 한다.
    - 투영성분 : $a^{\Vert b}$
    - 투영성분의 길이 : $\| a^{\Vert b} \| = \|a\|\cos\theta = \dfrac{\|a\|\|b\|\cos\theta}{\|b\|}  = \dfrac{a^Tb}{\|b\|} = \dfrac{b^Ta}{\|b\|} = a^T\dfrac{b}{\|b\|}$
    - 투영성분 그 자체 : $a^{\Vert b} = \dfrac{a^Tb}{\|b\|} \dfrac{b}{\|b\|}= \dfrac{a^Tb}{\|b\|^2}b$  \; ($\dfrac{b}{\|b\|}$ 는 벡터 b 의 단위벡터이다.)
    - b 가 단위 벡터 일 경우 : $\| a^{\Vert b} \| = a^Tb$ ; (단위벡터의 길이가 1이므로, a, b 의 내적이 된다.)
    - 직교성분 : $a^{\perp b}$
    - 직교성분은 투영성분을 뺸 나머지이다. : $a^{\perp b} = a - a^{\Vert b} $

#### 직선의 방정식
- 원점에서 출발하는 벡터 w 가 가리키는 지점을 지나고, 벡터 w 에 직교하는 어떤 직선 A.
- ```직선 A 위의 임의의 점 x 와 벡터 w 사이의 벡터.```
    - $w^T(x-w) = 0$, $w^Tx - w^Tw = w^Tx - \|w\|^2 = 0$,   (벡터 w 와 벡터 x-w 가 직교하므로)
    - 직선 A 와 원점 사이의 거리 : $\|w\|$  (벡터 w 의 길이)
- ```벡터 w 의 점을 지나지 않고, 벡터 w 에 직교인 직선 A 의 방정식```
    - 직선 A 는 벡터 w 의 점을 지나지는 않지만, 벡터 w 위의 어떤 점 cw 는 지난다. (c 는 임의의 상수)
    - $cw^Tx - c^2\|w\|^2 = w^Tx - c\|w\|^2 = 0$
    - cw 가 임의의 점이므로 $w^Tx - w_0 = 0$
    - 직선 A 와 원점 사이의 거리 : $c\|w\| = \dfrac{w_0}{\|w\|}$

#### 직선과 점의 거리
- 직선 A 가 $w^Tx - \|w\|^2 = 0$ 이고, 이 위에 있지 않은 임의의 점 x' 와의 거리
- x' 는 벡터 w 에 대해서 투영성분으로 분해 할 수 있다. 이 x' 의 벡터 w 에대한 투영성분으로 직선 A 와의 거리를 구한다.
    - 투영성분의 길이 : $\|x^{\Vert w}\| = \frac{w^Tx'}{\|w\|}$
    - 점 x' 와 직선 A 의 거리는 벡터 w 와 벡터 x' 의 투영성분의 차와 같다.
    - $\left|  \|x'^{\Vert w}\| - \|w\| \right| = \left| \dfrac{w^Tx'}{\|w\|} - \|w\| \right| = \dfrac{\left|w^Tx' - \|w\|^2 \right|}{\|w\|}$
- 직선 A 가 $w^Tx - \|w_0\| = 0$ 이고, 이 위에 있지 않은 임의의 점 x' 와의 거리
    - $\left|  \|x'^{\Vert w}\| - c\|w\| \right| = \dfrac{\left|w^Tx' - w_0 \right|}{\|w\|}$, ($w_0 = c\|w\|^2$ )
- 직선과 점사이의 거리는 분류방법의 하나인 ```서포트 벡터 머신 SVM, support vector machine``` 에서 사용된다.

### 2. 좌표와 변환

#### 선형종속과 선형독립
- ```선형종속 linearly dependent``` : 선형조합이 0이 되게끔 만드는 계수가 모두 0이 아닌 경우가 존재하는 경우. 즉 계수 c 가 0이 아니어도 선형조합의 값이 0이 되는 경우.
    - $c_{1}x_{1} + c_{2}x_{2} +  ...  + c_{N}x_{N} = 0$
- ```선형독립 linearly independent``` : 선형조합이 0이 되게끔 만드는 계수가 모두 0이어야만 하는 경우. 즉 계수 c 가 0이 아니면 선형조합의 값이 0이 안되는 경우.
    - $c_1 x_1 + \cdots + c_N x_N = 0  \;\; \rightarrow \;\; c_1 = \cdots = c_N = 0$ (반드시 계수 모두가 0 이어야 한다.)
    - $c_1 x_1 + \cdots + c_N x_N = 0  \;\; \leftrightarrow \;\; c_1 = \cdots = c_N = 0$ (계수 모두가 0 이면 선형조합이 0 이다라는 조건 추가)
- 모든 벡터들간에 선형독립이 성립하지 않는 경우 : 3 개의 2차원 벡터들, 4 개의 3차원 벡터들. 미지수의 갯수가 방정식의 갯수보다 많으므로 해의 갯수가 무한대이다. 즉 계수들이 0 이 아니어도 선형조합의 값이 0이 될 수 있는 경우가 많다.
- 선형독립인 벡터를 찾을 때 : 벡터의 요소별 비율이 같으면 선형독립이 된다.

#### 선형독립과 선형 연립방정식
- 선형독립 관계를 행렬의 선형연립방정식 형태로 나타낼 수 있다.
    - $c_{1}x_{1}+c_{2}x_{2}+...+c_{N}x_{N} = Xc$
    - 벡터의 선형독립 문제는 선형연립방정식을 푸는 것과 같다. : $Xc = 0$
    - $Xc \;\; \rightarrow \;\; c = 0$, $Xc \;\; \leftrightarrow \;\; c = 0$

#### 선형종속의 대표적인 예
- 데이터의 열벡터들이 선형종속이면 다중공선성 multicollinearity 라고 한다. 예측 성능이 떨어진다. 안 좋은 데이터.
- 벡터가 선형종속이 되는 대표적인 경우
    - ```벡터의 개수가 벡터의 차원보다 크면 선형종속이다.``` : 열이 행보다 많다. 특징이 데이터보다 많다. 미지수가 방정식보다 많다. 해가 많다.
    - ```중복 데이터가 있으면 반드시 선형종속이다.``` : i, j 번째 열벡터가 같으면 선형종속이다. c 가 0 이 아닌 경우에도 선형조합이 0 이 된다.
    - ```어떤 벡터가 다른 벡터의 선형조합이면 선형종속이다.``` : 주차별 매출이 각각 다른 벡터인데, 매출 평균이 다른 벡터에 들어 있는 경우.

#### 랭크
- 랭크 rank : 어떤 행렬 A 에서 서로 선형독립인 벡터들의 최대 갯수. 스칼라.
    - ```열랭크 column rank``` : 열벡터 간의 선형독립인 열벡터들의 최대 갯수
    - ```행랭크 row randk``` : 행벡터 간의 선형독립인 행벡터들의 최대 갯수
- 랭크의 성질
    - ```행랭크와 열랭크는 같다.``` 즉 행 기준으로 선형조합을 따지든, 열 기준으로 선형조합을 따지든 선형독립 벡터의 최대갯수는 같다.
    - 행랭크는 행의 갯수보다 커질 수 없고, 열벡터는 열의 갯수보다 커질 수 없다.
    - $rankA \leq min(M,N)$ (M,N 중 작은 것과 같거나 작다.)
    - 랭크가 1인 경우도 있다.
- ```풀랭크 full rank``` : 랭크가 행이나 열의 갯수 중 작은 값과 같으면 풀랭크라고 한다.
    - $\text rankA = min(M,N)$ (M,N 중 작은 것과 같다.)
    - 벡터간의 선형조합의 값이 0 인 계수 c1,c2...cn 이 모두 0 일 때만 가능한 경우 ```선형독립```이라고 하며, 어떤 행렬에서 선형독립인 벡터들의 최대 갯수를 ```랭크```라고 한다. ```풀랭크```는 이러한 랭크의 갯수가 행과 열의 갯수 중 작은 값과 같은 경우를 말한다.
    - 선형독립인 벡터들로 행렬을 만들었을 때 항상 풀랭크이다.
    - 풀랭크 인 행렬일 수록 좋은 데이터이다.
    - 위의 성질에 따라서 4x3 행렬에서 랭크는 3을 넘을 수 없고, 행과 열의 랭크가 같으므로, 3개의 열에서 랭크의 갯수를 찾는 것이 효율적이다. 랭크가 2가 나오면 풀랭크가 아니다. 최소 3개가 나와야 풀랭크 행렬이다.
- ```로우 랭크 행렬 low rank matrix```
    - N 차원 벡터 x 한 개로 만들어지는 정방 행렬을 ```랭크-1 행렬``` 이라고 한다. : $xx^T \in\mathbf R^{N\times N} $
    - 벡터 x 는 기본적으로 열벡터이므로 열벡터와 행벡터의 곱의 형태이므로 정방행렬이 된다.
        - 랭크-1 행렬의 랭크는 1이다. 열벡터, 행벡터가 곱해져서 정방행렬로 뻥튀기 된 것. 쓸모 있는 것은 하나밖에 없다.
    - N 차원 벡터 x 두 개로 만들어지는 다음 행렬을 ```랭크-2 행렬``` 이라고 한다. : $x_1x_1^T + x_2x_2^T $
        - 랭크-2 행렬의 랭크는 2이다.
    - N 차원 벡터 x M 개로 만들어지는 다음 행렬을 ```랭크-M 행렬``` 이라고 한다. : $x_1x_1^T + x_2x_2^T + ... + x_Mx_M^T = \sum_{i=1}^Mx_ix_i^T$
        - 랭크-M 행렬은 ```특잇값 분해와 PCA principal component analysis 에서 사용 된다.```
- ```정방행렬 X 가 풀랭크 이면 역행렬이 존재한다.``` 풀랭크의 여부로 역행렬이 있는지 없는지 확인 할 수 있다.
    - 정방행렬이 풀랭크 이면, 행과 열 벡터 모두가 선형독립이다.
    - 역행렬이 존재한다면, 선형회귀모델에서 가중치 x 의 값을 구할 수 있다. $x=A^{-1}b$

#### 벡터공간과 기저벡터
- ```벡터공간 vector space``` : 서로 선형독립인 벡터 N 개를 선형조합하여 만들어지는 벡터의 집합을 벡터공간이라고 한다. $V$
    - 벡터공간의 차원 : 벡터공간의 차원은 벡터공간을 이루는 벡터의 갯수 N 개 (벡터의 차원은 벡터의 원소의 수)
    - 벡터 100개를 선형조합하여 만든 벡터공간 V 의 차원은 100차원이다.
    - 서로 선형독립인 벡터 N 를 벡터공간의 ```기저벡터 basis vector``` 라고 한다.
    - $V = \{c_1x_1 + \cdots + c_Nx_N \; \vert \; c_1, \ldots, c_N \in\mathbf {R} \} $
- N 차원 벡터 N 개가 선형독립이면 아래의 정리가 성립한다.
    - N 개의 N 차원 벡터 $x_1,x_2,\cdots,x_N$ 이 선형독립이면, 이를 선형조합하여 모든 N 차원 벡터를 만들 수 있다.
    - x_1=np.array([[1],[2]]), x_2=np.array([[2],[1]]) 두 벡터는 서로 선형독립이다. 이 정리에 따라서 두 벡터를 계수 c_1, c_2 와 선형조합하여 어떠한 2 차원 벡터도 만들 수 있다. 이러한 선형조합하여 만들어진 벡터들의 집합을 벡터공간이라고 한다.
    - 선형독립이 아닌 벡터는 벡터공간의 기저벡터가 될 수 없다.
    - x_1=np.array([[1],[2],[0]]), x_2=np.array([[2],[1],[0]]) 은 선형독립이며 벡터공간의 기저벡터이다. 이 벡터공간은 2개의 벡터로 만들어졌으므로 2차원이다. (3차원이 아니다.)
- 벡터공간의 차원과 벡터의 차원의 기준이 다른 이유는 기저벡터를 선형조합하여 만들지 못하는 벡터들이 있기 때문.

#### 랭크와 역행렬
- 정방행렬의 랭크와 역행렬 사이의 정리.
    - ```정방행렬이 풀랭크면 역행렬이 존재한다. 역도 성립한다. 즉, 정방행렬의 역행렬이 존재하면 풀랭크이다.```
    - $\text{정방행렬이 풀랭크이다.} \; \leftrightarrow \; \text{역행렬이 존재한다.}$
    - -> 방향의 증명 : 기저벡터의 선형조합으로 항등행렬을 만들 수 있다고 가정하면, $XC=CX=I$ 가 성립한다. 역행렬의 성질에 의해서 C 는 X 의 역행렬이 된다.
    - <- 방향의 증명 : 선형연립방정식과 선형독립의 관계에서 벡터 N 개가 선형독립 일때의 논리기호 $Xc=0 \; \leftrightarrow \; c=0$ 를 사용한다. c=0 이면 Xc=0 성립, X의 역행렬이 존재한다고 가정하면 $X^{-1}Xc=c=0$ 이 성립한다. 따라서 X 의 역행렬이 존재한다.
- 어떤 정방행렬 데이터가 풀랭크이면 역행렬이 존재하고, 역행렬이 존재하면 선형조합의 해와 선형연립방정식의 최소자승문제의 해를 의사역행렬을 통해서 구할 수 있게 된다.

#### 벡터공간과 투영벡터, 직교벡터
- N 차원 벡터 M 개 v1, v2, ... , vm 으로 이루어진 기저벡터가 있을 때, N 차원 벡터 x 와 이 기저벡터 v1,v2,...,vm 을 선형조합하여 벡터 $x^{\Vert v}$ 를 만들었다.
- 이 벡터와 벡터 x 의 차 $x-x^{\Vert v}$ 인 벡터 a가 모든 기저벡터 v1,v2,...,vm 에 대하여 직교할 때,
    - $x-x^{\Vert v}=x^{\perp v}$ 벡터를 ```벡터공간 V 에 대한 직교벡터```라고 한다.
    - $x^{\Vert v}$ 벡터를 ```벡터공간 V 에 대한 투영벡터```라고 한다.
    - $x-x^{\Vert V} \perp \{v_1,v_2,\cdots,v_M\}$
- v1,v2,...,vm 은 기저벡터이므로 이미 선형독립인 벡터들이다.
- M=2, N=3 으로 정의하면, 2차원 평면에 3차원 벡터가 투영 된 것을 확인 할 수 있다.

#### 정규직교인 기저벡터로 이루어진 벡터공간
- 기저벡터 $v_1,v_1,...,v_M$ 이 정규직교이면, 투영벡터는 각 기저벡터에 대한 내적값으로 표현할 수 있다.
    - $x^{\Vert V}=(x^Tv_1)v_1+(x^Tv_2)v_2+\cdots+(x^Tv_M)v_M$
    - ```!!! 투영벡터가 내적의 결과라는 것은 알겠는데, 내적에서 v_1 이 왜 두번 곱해지는지 확인 할것, 이해 안됨.```
- 이러한 투영벡터의 길이의 제곱은 각 기저벡터와의 내적의 제곱합이다.
    - $\|x^{\Vert V}\|^2=\sum_{i=1}^M(x^Tv_i)^2$, (위의 식 정리)
    - 벡터 x 에서 투영벡터를 뺴면 직교벡터가 된다. (증명 가능)
- 직교벡터 $x^{\perp V}$ 는 기저벡터 $v_1,\cdots,v_M$ 으로 이루어진 벡터공간의 모든 벡터에 대해서 직교한다.
- 따라서 벡터 x 의 투영벡터 $x^{\Vert V}$ 는 기저벡터 $v_1,\cdots,v_M$ 이루어진 벡터공간의 모든 벡터 중에서 벡터 x 와 가장 가까운 벡터이다.

#### 표준기저벡터
- 표준기저벡터 standard basis vector : 기저벡터 중에서 원소 중 하나의 값이 1 이고 나머지는 0 으로 이루어진 것.
    - 표준기저벡터들을 열벡터로 갖는 행렬은 항등행렬이 된다.
    - $[e_1,e_2,\cdots,e_N] = I_N$

####  좌표
- ```좌표 coordinate``` : 어떤 벡터 x 를 나타내기 위해 기저벡터를 선형조합하여 만든 계수벡터를 말한다.
    - $x = x_{e_1} e_1 + x_{e_2} e_2$
- 어떤 벡터 x 가 있을 때 이 벡터의 위치를 표시하는 것은 어떤 기저벡터를 기준으로 했느냐에 따라 달라진다. 이러한 기준이 되는 기저벡터로 벡터 x 의 위치를 표시한 것을 좌표라고 한다.
    - $x = [ e_1 e_2 ] \begin{bmatrix} x_{e_1} \\ x_{e_2} \end{bmatrix} = [ e_1 e_2 ] \; x_e$ (벡터 x 를 기저벡터로 나타낸 것)
    - $x_e = \begin{bmatrix} x_{e_1} \\ x_{e_2} \end{bmatrix}$ (x_e 가 벡터 x 에 대한 기저벡터의 좌표가 된다.)
- 기저벡터가 바뀌면 벡터 x 는 그대로 이지만 좌표는 새 기저벡터에 맞게 바뀐다. 즉 기존의 기저벡터 e 에서의 좌표와 새로운 기저벡터 g 에서의 좌표는 다르다.
    - $g_1 = \begin{bmatrix} 1 \\ 0 \end{bmatrix}, \; g_2 = \begin{bmatrix} -1 \\ 1 \end{bmatrix}$ (새로운 기저벡터)
    - $x_g = \begin{bmatrix} 4 \\ 2 \end{bmatrix}$ (벡터 x 에 대한 새로운 기저벡터의 좌표)
    - 이 의미는 벡터 x 는 g_1 방향으로 4 만큼, g_2 방향으로 2 만큼 이동한 벡터의 합이 가리키는 지점이라는 뜻이다.
- 그러나 같은 벡터 x 에 대한 상대적인 값이기 때문에 기저벡터간의 호환할 수 있는 기준이 필요하다. 즉 기존 기저벡터와 새로운 기저벡터를 서로를 사용해서 정의할 수 있어야 한다.
    - $\begin{bmatrix} g_1 & g_2 \end{bmatrix}= \begin{bmatrix} e_1 & e_2 \end{bmatrix} \begin{bmatrix} g_{1e} & g_{2e} \end{bmatrix}
= \begin{bmatrix} e_1 & e_2 \end{bmatrix} A$
    - g1e, g2e 를 열벡터로 묶은 행렬 A
- ```변환행렬 transform matrix``` : 벡터 x 를 나타내는데 기존의 기저벡터와 새로운 기저벡터를 서로 호환하기 위한 행렬 A 의 역행렬 $A^-1=T$
- ```좌표변환 coordinate transform``` : 새로운 기저벡터에 대해 좌표를 계산하는 것
- 벡터 x 의 기저벡터 $\{e_1, e_2\}$ 의 좌표 xe 를 새로운 기저벡터 $\{g_1,g_2\}$ 에 대한 좌표 xg 로 변환하면서 변환행렬이 정의 된다.
    - $x = x_{e1}e_1 + x_{e2}e_2 = x_{g1} g_1 + x_{g2} g_2$
    - $\begin{bmatrix} e_1 & e_2 \end{bmatrix} x_e = \begin{bmatrix} g_1 & g_2 \end{bmatrix} x_g $
    - $\begin{bmatrix} g_1 & g_2 \end{bmatrix} = \begin{bmatrix} e_1 & e_2 \end{bmatrix}$ 을 대입한다.
    - $x_e = A x_g, \;\; x_g = A^{-1}x_e = Tx_e$

#### 이미지변환
- 좌표의 변환으로 이미지를 변환 할 수 있다.
    - 회전 : 기준이 되는 기저벡터가 바뀌면서 이미지의 방향이 바뀐다.
    - 스케일 : 기존 좌표에서 변환 된 좌표와의 비례에 따라서 이미지가 늘어나거나 줄어든다.
- 원점 자체를 바꿀 수 는 없다.
- scipy 패키지의 기저벡터 : $e_1=\begin{bmatrix} 0 \\ -1 \end{bmatrix}$, $e_2=\begin{bmatrix} 1 \\ 0 \end{bmatrix}$

### 3. 고윳값분해
- 고윳값분해와 특잇값분해는 행렬의 내부구조를 살펴보거나, 행렬의 연산을 효율적으로 하기위해 유용하게 사용된다.
- 행렬의 좌표변환의 일종이다. 변환해서 방향은 안 바뀌고 길이에만 변화가 있는 벡터.

#### 고윳값과 고유벡터
- ```고유벡터 eigenvector``` : 정방행렬 A 를 곱해서 변환하려고 해도 변환되지 않는 벡터. 영벡터가 아니어야한다. $v$
- ```고윳값 eigenvalue``` : 고유벡터의 변형 전,후의 크기의 비율. $\lambda$
- ```고윳값분해 eigenvalue decomposition, 고유분해 eigen-decomposition``` : 정방행렬 A 에서 고유벡터와 고윳값을 찾는 행위
- 행렬 A 에 대하여 고윳값과 고유벡터는 다음 식을 만족한다.
    - $Av=\lambda v$
    - $(A-\lambda I)v=0$
- 어떤 벡터 v 가 고유벡터이면, 이 벡터에 실수를 곱한 모든 cv 벡터, v와 방향이 같은 벡터는 모두 고유벡터가 된다.
    - $A(cv)=cAv=c \lambda v=\lambda(cv)$
    - 고유벡터는 길이가 1인 단위벡터로 정규화 하여 표시한다. : $\dfrac{v}{\|v\|}$

#### 특성방정식
- 정방행렬 A 만 주어졌을 때 고윳값과 고유벡터를 구하는 방법.
- 고윳값은 $A-\lambda I$ 의 행렬식의 값을 0으로 만드는 특성방정식 characteristic equation 의 해와 같다.
    - $det(A-\lambda I)=0$
    - 이 조건에 의해서 $A-\lambda I$ 는 역행렬을 갖지 않는다는 것을 알 수 있다. (역행렬 공식 확인)
    - 특성방정식의 해를 구하면 고윳값을 구할 수 있고, 행렬과 고윳값을 알고 있기 때문에 고유벡터도 구할 수 있게 된다.
    - 고윳값은 특성방정식의 해에 따라서 1개 (중복고윳값), 2개, 또는 실수가 아닌 복소수인 경우 로 달라지게 된다.

#### 고윳값의 갯수
- ```중복된 고윳값을 가진 경우 각각으로 생각하고, 복소수인 고윳값도 가능하다면, n 차원 정방행렬의 고윳값은 항상 n 개이다.```

#### 고윳값과 대각합/행렬식
- 대각합과 행렬식은 벡터와 행렬의 크기에 해당하는 개념들.
- 어떤 행렬의 고윳값이 $\lambda_1, \lambda_2, \cdots, \lambda_n$ 라고 할 떄, 모든 고윳값의 합은 행렬의 대각합과 같고, 모든 고윳값의 곱은 행렬의 행렬식 값과 같다.
    - $tr(A)=\sum_{i=1}^N \lambda_i$
    - $det(A)=\prod_{i=1}^N \lambda_i$
    - ```역행렬의 존재여부는 고윳값 중에 0 이 있느냐 없느냐로 판단 가능하다. 고윳값 중에 0 이 있으면, det(A) 가 0이 된다.```

#### 고유벡터의 계산
- 고윳값을 알면 다음 연립방정식을 풀어 고유벡터를 구할 수 있다.
    - $(A-\lambda I)v=0$
    - 고윳값 각각 고유벡터를 구해야 한다.
    - 고윳값이 1개일때 고유벡터도 1개이거나 경우에 따라서 여러개 일 수 있다. v_11 = 2/3v_12 를 만족하는 모든 벡터 가 될 수도 있다.
    - 항등행렬 I 의 고윳값은 1 하나이지만, 고유벡터는 임의의 2차원 벡터 모두가 될 수 있다.
    - 단, 단위벡터는 1개 이다.
    - 고유벡터는 단위벡터로 정규화하여 나타내주는 것이 좋다.

#### 대각화
- ```대각화 diagonalization``` : N 차원 정방행렬을 인수분해 하는 행위.
    - 고유벡터행렬 $V$ : 고유벡터를 열벡터로 옆으로 쌓아서 만든 행렬, $V \in\mathbf {R}^{N\times N}$, 정방행렬
    - 고유값행렬 $\Lambda$ : 고윳값을 대각성분으로 가지는 대각행렬, $\Lambda \in\mathbf {R}^{N\times N}$, 정방행렬
    - $AV=\Lambda V$
    - V 의 역행렬이 존재한다면 : $A=V\Lambda V^{-1}$
- ```정방행렬에 대한 고윳값-고유벡터의 성질```
    - 고윳값은 복소수이다.
    - 고윳값은 N 개이다. (N 차원 정방행렬의 고윳값은 N 개이다.)
    - $AV=\Lambda V$
    - $trA=\sum_{i=1}^N\lambda_i$ , (대각합은 고윳값들의 합과 같다.)
    - $detA=\prod_{i=1}^N\lambda_i$ , (행렬식은 고윳값들의 곱과 같다.)

#### 대각화 가능
- ```행렬이 대각화가능 하려면 고유벡터가 선형독립이어야 한다.```
    - 고유벡터행렬 V 가 역행렬이 있어야 하므로, 역행렬의 조건인 풀랭크이어야 한다. 정방행렬의 풀랭크 조건은 벡터들이 선형독립이어야 한다.

#### 고윳값과 역행렬
- ```대각화가능한 행렬에 0인 고윳값이 없으면 항상 역행렬이 존재한다.```
    - $A^{-1}=(V\Lambda V^{-1})^{-1}=V\Lambda^{-1} V^{-1}$
    - 고윳값행렬은 고윳값의 대각행렬이다. 대각행렬의 역행렬은 대각요소들의 역수이므로, 고윳값이 0이 있으면 역행렬을 만들 수 없다.
    - 또한 A 의 행렬식은 고윳값의 곱과 같다는 정의에 의해서 고윳값 중에 0 이 있으면, 행렬식 값이 0 이므로 역행렬이 존재하지 않는다.

#### 대칭행렬의 고유분해
- 대칭행렬의 성질 : $S^T=S,\,\, S\in\mathbf {R}^{N\times N}$ , (정방행렬만 대칭행렬이 가능하다.)
- ```행렬 A가 실수인 대칭행렬이면, 고윳값이 실수이고, 고유벡터는 서로 직교한다.```
    - 고유벡터가 단위벡터로 정규화 된 상태이면, 고유벡터행렬 V 는 정규직교 행렬이다.
    - 정규직교 행렬의 성질 : $V^TV=VV^T=I$ , $V^{-1}=V^T$ , ($v_i^Tv_j=0 \;\; (i\neq j)$, $v_i^Tv_j=1 \;\; (i=j)$)
    - $A=V\Lambda V^T$
    - ```따라서 실수인 대칭행렬은 항상 대각화가능하다.```

#### 대칭행렬과 랭크-1 행렬
- N 차원 대칭행렬 A 를 N 개의 랭크-1 행렬의 합으로 나타낼 수 있다.
- 로우랭크에 속하는 랭크-1행렬은 N 차원 벡터 1개로 만들 수 있는 정방행렬이다. rank=1
    - $A_i=v_iv_i^T$
    - $A=\sum_{i=1}^N\lambda_i v_iv_i^T=\sum_{i=1}^N\lambda_iA=\lambda_1 A_1+\cdots+ \lambda_N A_N$
- 만약 고윳값 중에 0 이 없다면, 역행렬도 랭크-1 행렬로 나타낼 수 있다.
    - $A^{-1}=V\Lambda^{-1} V^T=\sum_{i=1}^N \dfrac{1}{\lambda_i}v_iv_i^T=\dfrac{1}{\lambda_1}A_1+\cdots+\dfrac{1}{\lambda_N}A_N$

#### 대칭행렬의 고윳값 부호
- 대칭행렬을 랭크-1 행렬의 합으로 나타낼 수 있고, 고유벡터가 서로 직교한다는 성질을 사용하면 다음 정리가 가능하다.
- ```각 정리에 대한 증명 중요!!!```
    - 대칭행렬이 양의 정부호이면, 모든 고윳값이 양의 정부호이다. 역도 성립한다. : $PS \leftrightarrow \lambda_i>0$
    - 대칭행렬렬이 양의 준정부호이면, 모든 고윳값은 0 이거나 양수이다. 역도 성립한다. : $PSD \leftrightarrow \lambda_i \ge 0$
    - 양의 준정부호 정리에서 실제로 고윳값은 0 이 될 수 없다. 벡터 x 와 고유벡터 vi 가 직교할 때 0 이되는데, 고유벡터 v 집합은 N 차원 벡터공간에서 기저벡터를 이루기때문에 모든 기저벡터와 직교하는 벡터는 존재하지 않는다. 따라서 양의 정부호이다.
- ```대칭행렬에 대한 고윳값-고유벡터의 성질```
    - 고윳값은 실수이다.
    - 고유벡터행렬의 전치연산은 역행렬과 같다. : $V^T=V^{-1}$
    - 행렬 A 는 고유벡터행렬과 고윳값행렬, 고유벡터행렬의 전치연산의 곱이다. : $A=V\Lambda V^T$
    - 행렬 A 는 랭크-1 행렬의 합으로 나타낼 수 있다. : $A=\sum_{i=1}^N\lambda_i v_iv_i^T$

#### 분산행렬
- ```분산행렬 scatter matrix``` : 임의의 실수 행렬 X 에 대하여 $X^TX$ 인 정방행렬. 확률분포에서 사용된다.
- 분산행렬의 정리
    - ```분산행렬은 양의 준정부호이고 고윳값은 0보다 크거나 같다.```
    - 분산행렬의 이차형식으로 증명 : $x^T(X^TX)x=(Xx)^T(Xx)=u^Tu=\|u\|^2 \ge 0$
    - 분산행렬의 이차형식을 정리하면 어떤 행렬 u 의 제곱합이 되는데, 제곱합은 0 보다 크거나 같으므로 준정부호이다.
    - 또한 분산행렬은 대칭행렬이므로, 대칭행렬이 양의 준정부호이면 고윳값들은 모두 0 보다 크거나 같다.

#### 분산행렬의 역행렬
- ```행렬 X 가 풀랭크이면 이 행렬의 분산행렬의 역행렬이 존재한다.```
    - 행렬 X 가 풀랭크이면 열벡터들이 선형독립이면서 벡터공간의 기저벡터이다.
    - 행렬 x 의 열벡터인 v 벡터가 영벡터가 아니라면, Xv=u 가 성립하며 이때 어떤 벡터 u 는 영벡터가 아니다.
    - u 를 영벡터로 만드는 v 벡터가 영벡터가 아닌 벡터라고 한다면 선형독립이 아니게 된다. 따라서 X 가 풀랭크이므로 v 는 선형독립하고, v가 영벡터가 아니면 u 도 영벡터가 아니다. 그러므로 분산행렬의 이차형식은 양의 정부호가 된다.
    - $x^T(X^TX)x=(Xx)^T(Xx)=u^Tu > 0$
    - ```분산행렬이 양의 정부호이면 항상 역행렬이 존재한다.```
        - 분산행렬은 대칭행렬이므로 대칭행렬이 양의 정부호이면 고윳값은 모두 0보다 크다.
        - 고윳값이 모두 양수이면 모든 고윳값의 곱이 양수이며 0보다 크다.
        - 정방행렬의 성질에 의해서 행렬식의 값이 모든 고윳값의 곱과 같으므로, 행렬식은 0 보다 크다. 따라서 역행렬이 존재한다.
- 역행렬이 존재하면 대칭행렬은 항상 양의 정부호인가?
    - 그렇지 않다. 역행렬이 존재한다면 행렬식이 0 이 아닌 양수이거나 음수이다. 행렬식이 음수인 경우라면, 고윳값의 곱이 음수이므로, 고윳값 중 한 개 이상이 음수가 된다.
    - 고윳값과 대칭행렬의 부호 성질에 의해서 고윳값이 음수이면 대칭행렬은 양의 정부호가 아니다.
    - 그러므로 역행렬이 존재한다고 해서 대칭행렬이 항상 양의 정부호 인것은 아니다.
    - 대칭행렬이 양의 정부호이면 항상 역행렬이 존재한다는 맞다.

#### 고유분해의 성질 요약
- 고유분해와 관련된 정리들은 데이터분석에서 자주 사용되므로 잘 익혀야 한다.
- N 차원 정방행렬 A 에 대해서 다음과 같은 사항이 성립한다.
    - 행렬 A는 N개의 고윳값-고유벡터를 갖는다. (복소수인 경우와 중복고윳값인 경우 포함)
    - 행렬의 대각합은 모든 고윳값의 합과 같다.
    - 행렬의 행렬식은 모든 고윳값의 곱과 같다.
    - 행렬 A가 대칭행렬이면 실수 고윳값 N 개를 가지며 고유벡터들이 서로 직교한다.
        - 정규직교하는 벡터의 성질은 증명에서 잘 사용된다. $V^TV=I,\;\;V^T=V^{-1},\;\;v_i^Tv_j=0(i \neq j),\;\;v_i^Tv_j=1(i=j)$
    - 행렬 A가 대칭행렬이고 고윳값이 모두 양수이면 양의 정부호이고 역행렬이 존재한다. 역도 성립한다.
    - 행렬 A가 어떤 행렬 X의 분산행렬이면 0 또는 양의 고윳값을 가진다.
    - 행렬 X가 풀랭크이면 분산행렬의 역행렬이 존재한다.

